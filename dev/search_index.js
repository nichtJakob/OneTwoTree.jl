var documenterSearchIndex = {"docs":
[{"location":"demo_classification/","page":"Demo Classification","title":"Demo Classification","text":"In this example we want to distinguish between 3 types of the iris flower.","category":"page"},{"location":"demo_classification/","page":"Demo Classification","title":"Demo Classification","text":"If you want to execute this example in your julia REPL you will first need to install some dependencies:","category":"page"},{"location":"demo_classification/","page":"Demo Classification","title":"Demo Classification","text":"julia>  using Pkg\njulia>  Pkg.add(\"MLDatasets\")\njulia>  Pkg.add(\"DataFrames\")\njulia>  Pkg.add(url=\"https://github.com/nichtJakob/OneTwoTree.jl.git\")","category":"page"},{"location":"demo_classification/","page":"Demo Classification","title":"Demo Classification","text":"First we load the iris dataset. Targets are the 3 types of the flowers and data contains measurements of flowers.","category":"page"},{"location":"demo_classification/","page":"Demo Classification","title":"Demo Classification","text":"using OneTwoTree\nusing DataFrames\nusing MLDatasets: Iris\n\ndataset = Iris()\ndata = Array(dataset.features)\ntargets = String.(Array(dataset.targets))","category":"page"},{"location":"demo_classification/","page":"Demo Classification","title":"Demo Classification","text":"Let's have a look at the types of the iris flower contained in targets and the features, aka the given measurements of flowers.","category":"page"},{"location":"demo_classification/","page":"Demo Classification","title":"Demo Classification","text":"println(\"The possible targets are: \", unique(targets), \"\\n\")\nprintln(\"The measured features are: \", names(dataset.features), \"\\n\")\nprintln(\"Size of data: \", size(data), \"\\n\")","category":"page"},{"location":"demo_classification/","page":"Demo Classification","title":"Demo Classification","text":"We have 150 data points as you can see in the size of the data.","category":"page"},{"location":"demo_classification/","page":"Demo Classification","title":"Demo Classification","text":"Now we define how big training and test set should be. You can modify the splitting point to be any value between 1 and 149.","category":"page"},{"location":"demo_classification/","page":"Demo Classification","title":"Demo Classification","text":"splitting_point = 120\n\nif splitting_point < 1 || splitting_point > 150\n    error(\"You have chosen an invalid splitting point. Please choose a value between 1 and 150\")\nend","category":"page"},{"location":"demo_classification/","page":"Demo Classification","title":"Demo Classification","text":"We split the data in training and test sets.","category":"page"},{"location":"demo_classification/","page":"Demo Classification","title":"Demo Classification","text":"train_data = data[1:splitting_point, :]\ntrain_labels = targets[1:splitting_point]\ntest_data = data[splitting_point+1:150, :]\ntest_labels = targets[splitting_point+1:150]\n\nprintln(\"Size of train data: \", size(train_data), \"\\n\")\nprintln(\"Size of test data: \", size(test_data), \"\\n\")","category":"page"},{"location":"demo_classification/","page":"Demo Classification","title":"Demo Classification","text":"We use the OneTwoTree package to build a decision tree, you can experiment with different tree-depths.","category":"page"},{"location":"demo_classification/","page":"Demo Classification","title":"Demo Classification","text":"our_max_depth = 3\ntree = DecisionTreeClassifier(max_depth=our_max_depth)","category":"page"},{"location":"demo_classification/","page":"Demo Classification","title":"Demo Classification","text":"Now we train on the training data.","category":"page"},{"location":"demo_classification/","page":"Demo Classification","title":"Demo Classification","text":"fit!(tree, train_data, train_labels)\n\nprintln(\"\\n\\nOur Tree:\\n\")\nprint_tree(tree)","category":"page"},{"location":"demo_classification/","page":"Demo Classification","title":"Demo Classification","text":"Let's see how good our tree is at predicting labels of data points in the test data.","category":"page"},{"location":"demo_classification/","page":"Demo Classification","title":"Demo Classification","text":"test_predictions = predict(tree, test_data)\naccuracy = sum(test_predictions .== test_labels) / length(test_labels)\n\nprintln(\"\\n\\nFor the Iris dataset we have achieved a test accuracy of $(round(accuracy * 100, digits=2))%\")\nprintln(\"-----------------------------------------------------\\n\")","category":"page"},{"location":"demo_classification/","page":"Demo Classification","title":"Demo Classification","text":"Now let's try using random forests.","category":"page"},{"location":"demo_classification/","page":"Demo Classification","title":"Demo Classification","text":"n_features_per_tree = 30 # number of features per tree\nprintln(\"\\n\\nNow we will grow our random forest containing 5 trees with $n_features_per_tree\nfeatures per tree and a max depth of $our_max_depth\")\n\nforest = ForestClassifier(n_trees=5, n_features_per_tree=n_features_per_tree, max_depth=our_max_depth)\nfit!(forest, train_data, train_labels)\n\nprint_forest(forest)","category":"page"},{"location":"demo_classification/","page":"Demo Classification","title":"Demo Classification","text":"Let's see how good our tree is at predicting lables of datapoints in the test data.","category":"page"},{"location":"demo_classification/","page":"Demo Classification","title":"Demo Classification","text":"forest_test_predictions = predict(forest, test_data)\nforest_accuracy = sum(forest_test_predictions .== test_labels) / length(test_labels)\n\nprintln(\"\\n\\nFor the Iris dataset the forest has achieved a test accuracy of $(round(forest_accuracy * 100, digits=2))%\")","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"Modules = [OneTwoTree]","category":"page"},{"location":"functions/#OneTwoTree.AbstractForest","page":"Functions","title":"OneTwoTree.AbstractForest","text":"AbstractForest\n\nBase type for classification and regression forests.\n\nForests are collections of trees which aggregate their decisions. We use this abstract type to differentiate tree construction intricacies in the fit! function\n\n\n\n\n\n","category":"type"},{"location":"functions/#OneTwoTree.Decision","page":"Functions","title":"OneTwoTree.Decision","text":"Decision{S<:Union{Number, String}}\n\nA structure representing a decision with a function and its parameter.\n\nParameters\n\nfn::Function: The decision function.\nparam::Union{Number, String}: The parameter for the decision function.\nNumber: for comparison functions (e.g. x < 5.0)\nString: for True/False functions (e.g. x == \"red\" or x != 681)\nfeature::Int64: The index of the feature to compare.\n\n\n\n\n\n","category":"type"},{"location":"functions/#OneTwoTree.DecisionTreeClassifier","page":"Functions","title":"OneTwoTree.DecisionTreeClassifier","text":"DecisionTreeClassifier <: AbstractDecisionTree\n\nA DecisionTreeClassifier is a tree of decision nodes. It can predict classes based on the input data. In addition to a root node it holds meta informations such as max_depth etc. Use fit(tree, features, labels) to create a tree from data.\n\n\n\n\n\n","category":"type"},{"location":"functions/#OneTwoTree.DecisionTreeClassifier-Tuple{}","page":"Functions","title":"OneTwoTree.DecisionTreeClassifier","text":"DecisionTreeClassifier(; root=nothing, max_depth=-1)\n\nInitialises a decision tree model.\n\nArguments\n\nroot::Union{Node, Nothing}: the root node of the decision tree; nothing if the tree is empty\nmax_depth::Int: maximum depth of the decision tree; no limit if equal to -1\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.DecisionTreeRegressor","page":"Functions","title":"OneTwoTree.DecisionTreeRegressor","text":"DecisionTreeRegressor\n\nA DecisionTreeRegressor is a tree of decision nodes. It can predict function values based on the input data. In addition to a root node it holds meta informations such as max_depth etc. Use fit(tree, features, labels) to create a tree from data.\n\n\n\n\n\n","category":"type"},{"location":"functions/#OneTwoTree.DecisionTreeRegressor-Tuple{}","page":"Functions","title":"OneTwoTree.DecisionTreeRegressor","text":"DecisionTreeRegressor(; root=nothing, max_depth=-1)\n\nInitialises a decision tree model.\n\nArguments\n\nroot::Union{Node, Nothing}: the root node of the decision tree; nothing if the tree is empty\nmax_depth::Int: maximum depth of the decision tree; no limit if equal to -1\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.ForestClassifier","page":"Functions","title":"OneTwoTree.ForestClassifier","text":"ForestClassifier <: AbstractForest\n\nForest for classification problems\n\nFields:\n\ntrees::Vector{DecisionTreeClassifier}: A vector of decision trees.\nn_trees::Int: The number of trees in the forest.\nn_features_per_tree::Int: The number of randomly drawn features used for each tree.\nmax_depth::Int: The upper bound for the depth of each tree.\n\n\n\n\n\n","category":"type"},{"location":"functions/#OneTwoTree.ForestClassifier-Tuple{}","page":"Functions","title":"OneTwoTree.ForestClassifier","text":"ForestClassifier(;n_trees::Int, n_features_per_tree::Int, max_depth::Int)\n\nConstructs a ForestClassifier instance.\n\nArguments:\n\nn_trees::Int: The number of trees in the forest.\nn_features_per_tree::Int: The number of randomly drawn features used for each tree.\nmax_depth::Int: The upper bound for the depth of each tree.\n\nReturns:\n\nA ForestClassifier instance.\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.ForestRegressor","page":"Functions","title":"OneTwoTree.ForestRegressor","text":"ForestRegressor <: AbstractForest\n\nForest for regression problems\n\nFields:\n\ntrees::Vector{DecisionTreeRegressor}: A vector of regression trees.\nn_trees::Int: The number of trees in the forest.\nn_features_per_tree::Int: The number of randomly drawn features used for each tree.\nmax_depth::Int: The upper bound for the depth of each tree.\n\n\n\n\n\n","category":"type"},{"location":"functions/#OneTwoTree.ForestRegressor-Tuple{}","page":"Functions","title":"OneTwoTree.ForestRegressor","text":"ForestRegressor(;n_trees::Int, n_features_per_tree::Int, max_depth::Int)\n\nConstructs a ForestRegressor instance.\n\nArguments:\n\nn_trees::Int: The number of trees in the forest.\nn_features_per_tree::Int: The number of randomly drawn features used for each tree.\nmax_depth::Int: The upper bound for the depth of each tree.\n\nReturns:\n\nA ForestRegressor instance.\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.Node","page":"Functions","title":"OneTwoTree.Node","text":"Node\n\nA Node represents a decision in the Tree. It is a leaf with a prediction or has exactly one true and one false child and a decision function.\n\n\n\n\n\n","category":"type"},{"location":"functions/#Base.show-Tuple{IO, OneTwoTree.AbstractForest}","page":"Functions","title":"Base.show","text":"Displays a string representation of the forest when shown in the REPL or other I/O streams.\n\nArguments:\n\nio::IO: The I/O stream.\nforest::AbstractForest: The forest to be displayed.\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree._decision_to_string-Tuple{OneTwoTree.Decision{<:Number}}","page":"Functions","title":"OneTwoTree._decision_to_string","text":"_decision_to_string(d::Decision)\n\nReturns a string representation of the decision function.\n\nArguments\n\nd::Decision: The decision function to convert to a string.\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree._forest_to_string-Tuple{OneTwoTree.AbstractForest}","page":"Functions","title":"OneTwoTree._forest_to_string","text":"_forest_to_string(forest::AbstractForest)\n\nConverts the forest to a string.\n\nArguments:\n\nforest::AbstractForest: The forest to be represented.\n\nReturns:\n\nA string with numerated text representations of the trees in the forest\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree._node_to_string-Tuple{OneTwoTree.Node, Bool, String}","page":"Functions","title":"OneTwoTree._node_to_string","text":"_node_to_string(node::Node, is_true_child::Bool, indentation::String)\n\nRecursive helper function to stringify the decision tree structure.\n\nArguments\n\nnode::Node: The current node to print.\nis_true_child::Bool: Boolean indicating if the node is a true branch child.\nindentation::String: The current indentation.\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree._node_to_string_as_root-Tuple{OneTwoTree.Node}","page":"Functions","title":"OneTwoTree._node_to_string_as_root","text":"_node_to_string_as_root(node::Node)\n\nPrint the tree from the given node by considering it to be the root of the tree.\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree._tree_to_string","page":"Functions","title":"OneTwoTree._tree_to_string","text":"_tree_to_string(tree::AbstractDecisionTree, print_parameters=true)\n\nReturns a textual visualization of the decision tree.\n\nArguments\n\ntree::AbstractDecisionTree The DecisionTree instance to print.\nprint_parameters::Bool=true: (Optional) Whether to print the tree parameters like max_depth.\n\nExample output:\n\nx < 28.0 ?\n├─ False: x == 161.0 ?\n│  ├─ False: 842\n│  └─ True: 2493\n└─ True: 683\n\n\n\n\n\n","category":"function"},{"location":"functions/#OneTwoTree._verify_fit!_args-NTuple{4, Any}","page":"Functions","title":"OneTwoTree._verify_fit!_args","text":"_verify_fit!_args(tree, dataset, labels, column_data)\n\nSome guards to ensure the input data is valid for training a tree.\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.calc_accuracy-Tuple{AbstractArray, AbstractArray}","page":"Functions","title":"OneTwoTree.calc_accuracy","text":"calc_accuracy(labels::AbstractArray, predictions::AbstractArray)\n\nCalculates the accuracy of the predictions compared to the labels.\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.calc_depth-Tuple{OneTwoTree.AbstractDecisionTree}","page":"Functions","title":"OneTwoTree.calc_depth","text":"calc_depth(tree::AbstractDecisionTree)\n\nTraverses the tree and returns the maximum depth.\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.collect_classes-Tuple{AbstractMatrix, Vector{Int64}, Int64}","page":"Functions","title":"OneTwoTree.collect_classes","text":"collect_classes(dataset::AbstractMatrix, indices::Vector{Int64}, column::Int64)\n\nCollect all unique classes among a subset of the specified column of the dataset.\n\nArguments\n\ndataset::AbstractMatrix: the dataset to collect classes on\nindices::Vector{Int64}: the indices of the numeric labels to be considered\ncolumn::Int64: the index of the dataset column/feature to collect the classes on\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.entropy-Tuple{AbstractVector}","page":"Functions","title":"OneTwoTree.entropy","text":"entropy(labels::AbstractVector)::Float64\n\nThis function calculates the entropy of set of lables\n\nArguments:\n\nlabels: A vector of labels\n\nReturns:\n\nThe entropy H(X) = -sum^n{i=1} P(xi) log2(P(xi))   with X beeing the labels   and n the number of elements in X   and P() beeing the Probability\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.equal-Tuple{Any, String}","page":"Functions","title":"OneTwoTree.equal","text":"equal(x, class::String; feature::Int64 = 1)::Bool\n\nA basic categorical decision function for testing and playing around.\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.fit!-Union{Tuple{T}, Tuple{S}, Tuple{OneTwoTree.AbstractDecisionTree, AbstractMatrix{S}, AbstractVector{T}}} where {S, T<:Union{Real, String}}","page":"Functions","title":"OneTwoTree.fit!","text":"fit!(tree::AbstractDecisionTree, features::AbstractMatrix{S}, labels::AbstractVector{T}; splitting_criterion=nothing, column_data=false) where {S, T<:Union{Real, String}}\n\nTrain a decision tree on the given data using some algorithm (e.g. CART).\n\nArguments\n\ntree::AbstractDecisionTree: the tree to be trained\ndataset::AbstractMatrix{S}: the training data\nlabels::AbstractVector{T}: the target labels\nsplitting_criterion::Function: a function indicating some notion of gain from splitting a node. If not provided, default criteria for classification and regression are used.\ncolumn_data::Bool: whether the datapoints are contained in dataset columnwise\n\n(OneTwoTree provides the following splitting criteria for classification: ginigain, informationgain; and for regression: variance_gain. If you'd like to define a splitting criterion yourself, you need to consider the following:\n\nThe function must calculate a 'gain'-value for a split of a node, meaning that larger values are considered better.\nThe function signature must conform to my_func(parent_labels::AbstractVector, true_child_labels::AbstractVector, false_child_labels::AbstractVector) where parentlabels is a set of datapoint labels, which is split into two subsets truechildlabels & falsechildlabels by some discriminating function. (Each label in parentlabels is contained in exactly one of the two subsets.)\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.fit!-Union{Tuple{T}, Tuple{S}, Tuple{OneTwoTree.AbstractForest, AbstractMatrix{S}, AbstractVector{T}}} where {S, T<:Union{Real, String}}","page":"Functions","title":"OneTwoTree.fit!","text":"fit!(forest::AbstractForest, dataset::AbstractMatrix{S}, labels::AbstractVector{T}; splitting_criterion=nothing, column_data=false) where {S, T<:Union{Real, String}}\n\nTrains each tree in the forest on randomly drawn subsets of test features and corresponding test labels.\n\nArguments\n\nforest::AbstractForest: the forest to be trained\ndataset::AbstractMatrix{S}: the training data\nlabels::AbstractVector{T}: the target labels\nsplitting_criterion: a function indicating some notion of gain from splitting a node. If not provided, default criteria for classification and regression are used.\ncolumn_data::Bool: whether the datapoints are contained in dataset columnwise\n\n(OneTwoTree provides the following splitting criteria for classification: ginigain, informationgain; and for regression: variance_gain. If you'd like to define a splitting criterion yourself, you need to consider the following:\n\nThe function must calculate a 'gain'-value for a split of a node, meaning that larger values are considered better.\nThe function signature must conform to my_func(parent_labels::AbstractVector, true_child_labels::AbstractVector, false_child_labels::AbstractVector) where parentlabels is a set of datapoint labels, which is split into two subsets truechildlabels & falsechildlabels by some discriminating function. (Each label in parentlabels is contained in exactly one of the two subsets.)\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.get_random_features-Union{Tuple{T}, Tuple{S}, Tuple{AbstractMatrix{S}, AbstractVector{T}, Int64}} where {S, T<:Union{Real, String}}","page":"Functions","title":"OneTwoTree.get_random_features","text":"get_random_features(features::AbstractMatrix{S}, labels::AbstractVector{T}, n_features::Int) where {S, T<:Union{Real, String}}\n\nReturns random features and their corresponding labels from the given dataset.\n\nArguments:\n\nfeatures::AbstractMatrix{S}: The feature matrix.\nlabels::AbstractVector{T}: The labels vector.\nn_features::Int: The number of features to draw randomly.\n\nReturns:\n\nA tuple (random_features, random_labels) containing the randomly drawn features and labels.\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.gini_gain-Tuple{AbstractVector, AbstractVector, AbstractVector}","page":"Functions","title":"OneTwoTree.gini_gain","text":"gini_gain(parent_labels::AbstractVector, true_child_labels::AbstractVector, false_child_labels::AbstractVector)::Float64\n\nThis function calculates the gain in Gini impurity for a split in a decision tree. The split is characterized by the partition of the parentlabels into truechildlabels and falsechild_labels according to some discriminant function.\n\nArguments:\n\nparent_labels: A vector of data labels (e.g., classes or numerical values in the case of regression).\ntrue_child_labels: A vector of a subset of data labels contained in parent_labels.\nfalse_child_labels: A vector of a subset of data labels contained in parent_labels.\n\nReturns:\n\nThe gain in Gini impurity of the split.\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.information_gain-Tuple{AbstractVector, AbstractVector, AbstractVector}","page":"Functions","title":"OneTwoTree.information_gain","text":"information_gain(parent_labels::AbstractVector, true_child_labels::AbstractVector, false_child_labels::AbstractVector)::Float64\n\nThis function calculates the information gain for a split in a decision tree. The split is characterized by the partition of the parentlabels into truechildlabels and falsechild_labels according to some discriminant function.\n\nArguments:\n\nparent_labels: A vector of data labels (e.g., classes or numerical values in the case of regression).\ntrue_child_labels: A vector of a subset of data labels contained in parent_labels.\nfalse_child_labels: A vector of a subset of data labels contained in parent_labels.\n\nReturns:\n\nThe information gain of the split.\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.is_leaf-Tuple{OneTwoTree.Node}","page":"Functions","title":"OneTwoTree.is_leaf","text":"is_leaf(node::Node)::Bool\n\nDo you seriously expect a description for this?\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.label_mean-Union{Tuple{T}, Tuple{Vector{T}, Vector{Int64}}} where T<:Real","page":"Functions","title":"OneTwoTree.label_mean","text":"label_mean(labels::Vector{T}, indices::Vector{Int64}) where T<:Real\n\nCalculate the mean of a subset of numeric labels.\n\nlabels::Vector{<:Real}: the vector of numeric labels\nindices::Vector{Int64}: the indices of the numeric labels to be considered\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.less_than_or_equal-Tuple{Any, Float64}","page":"Functions","title":"OneTwoTree.less_than_or_equal","text":"less_than_or_equal(x, threshold::Float64; feature::Int64 = 1)::Bool\n\nA basic numerical decision function for testing and playing around.\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.most_frequent_class-Union{Tuple{T}, Tuple{Vector{T}, Vector{Int64}}} where T<:Union{Real, String}","page":"Functions","title":"OneTwoTree.most_frequent_class","text":"most_frequent_class(labels::Vector{T}, indices::Vector{Int64}) where T <: Union{Real, String}\n\nDetermine the most frequent class among a subset of class labels.\n\nArguments\n\nlabels::Vector{<:Union{Real, String}}: class labels\nindices::Vector{Int64}: the indices of the class labels, to be considered/counted\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.predict-Tuple{OneTwoTree.AbstractDecisionTree, Union{AbstractMatrix, AbstractVector}}","page":"Functions","title":"OneTwoTree.predict","text":"predict(tree::AbstractDecisionTree, X::Union{AbstractMatrix, AbstractVector})\n\nTraverses the tree for given datapoints and returns that trees prediction.\n\nArguments\n\ntree::AbstractDecisionTree: the tree to predict with\nX::Union{AbstractMatrix, AbstractVector}: the data to predict on\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.predict-Union{Tuple{S}, Tuple{OneTwoTree.AbstractForest, AbstractVecOrMat{S}}} where S<:Union{Real, String}","page":"Functions","title":"OneTwoTree.predict","text":"predict(forest::AbstractForest, X::Union{AbstractMatrix{S}, AbstractVector{S}}) where S<:Union{Real, String}\n\nOutputs the forest-prediction for a given datapoint X. The prediction is based on the aggregation of the tree decisions. For agregation in a regression scenario the mean is used. For agregation in a classification scenario the most voted class label is used.\n\nArguments:\n\nforest::AbstractForest: The trained forest.\nX::Union{AbstractMatrix{S}, AbstractVector{S}}: The input data for which a prediction is searched.\n\nReturns:\n\nPredictions for the input data X, aggregated across all trees in the forest.\n\nErrors:\n\nRaises an error if the forest contains no trained trees.\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.print_forest-Tuple{OneTwoTree.AbstractForest}","page":"Functions","title":"OneTwoTree.print_forest","text":"print_forest(forest::AbstractForest; io::IO=stdout)\n\nPrints the numerated trees of a forest.\n\nArguments:\n\nforest::AbstractForest: The forest to be printed.\nio::IO=stdout: (Optional) The I/O stream for printing\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.print_tree-Tuple{OneTwoTree.AbstractDecisionTree}","page":"Functions","title":"OneTwoTree.print_tree","text":"print_tree(tree::AbstractDecisionTree; io::IO=stdout)\n\nReturns a textual visualization of the decision tree.\n\nArguments\n\ntree::AbstractDecisionTree The DecisionTree instance to print.\nio::IO=stdout: (Optional) The I/O stream for printing\n\nExample output:\n\nx < 28.0 ?\n├─ False: x == 161.0 ?\n│  ├─ False: 842\n│  └─ True: 2493\n└─ True: 683\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.should_split-Tuple{OneTwoTree.Node, Float64, Int64}","page":"Functions","title":"OneTwoTree.should_split","text":"should_split(N::Node, splitting_gain::Float64, max_depth::Int64)\n\nDetermines whether to split the node N given.\n\nArguments\n\nN::Node: Node that may be split. N contains further fields relevant to the decision like the best splitting decision functtion and maximum tree depth.\nsplitting_gain::Float64: The gain in the splitting criterion metric of N after it's optimal split.\nmax_depth::Int64: The maximum depth of the tree N is part of.\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.split-Tuple{OneTwoTree.Node, Function}","page":"Functions","title":"OneTwoTree.split","text":"split(N::Node, splitting_criterion::Function)\n\nDetermine the optimal split of a node. Handles numerical and categorical data and labels.\n\nArguments\n\nN::Node: The node to be split. All additional information for the split calculation (e.g. dataset, labels, node_data) is contained in N.\nsplitting_criterion::Function: The function to calculate the gain of a split. See docstring of fit! for more information.\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.split_indices-Union{Tuple{T}, Tuple{AbstractMatrix, Vector{Int64}, Function, T, Int64}} where T<:Union{Real, String}","page":"Functions","title":"OneTwoTree.split_indices","text":"function split_indices(dataset::AbstractMatrix, node_data::Vector{Int64}, decision_fn::Function, decision_param::T, decision_feature::Int64) where {T<:Union{Real, String}}\n\nSplit the dataset indices contained in node_data into two sets via the decision function.\n\nArguments\n\ndataset::AbstractMatrix: the dataset to split on (the datapoints are assumed to be contained rowwise)\nnode_data::Vector{Int64}: the index list, indexing dataset, to be split\ndecision_fn::Function: the decision function taking as input a datapoint, decisionparam and decisionfeature and returning a Bool\ndecision_param<:Union{Real, String}: a parameter to the decision function. This can be a class name or a numeric threshold.\ndecision_feature::Int64: the index of the dimension of datapoints along which to split\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.variance_gain-Tuple{AbstractVector, AbstractVector, AbstractVector}","page":"Functions","title":"OneTwoTree.variance_gain","text":"variance_gain(parent_labels::AbstractVector, true_child_labels::AbstractVector, false_child_labels::AbstractVector)::Float64\n\nThis function calculates the variance gain for a split in a decision tree. The split is characterized by the partition of the parentlabels into truechildlabels and falsechild_labels according to some discriminant function.\n\nArguments:\n\nparent_labels: A vector of data labels (e.g., classes or numerical values in the case of regression).\ntrue_child_labels: A vector of a subset of data labels contained in parent_labels.\nfalse_child_labels: A vector of a subset of data labels contained in parent_labels.\n\nReturns:\n\nThe variance gain of the split.\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.verify_forest_args-Tuple{Int64, Int64, Int64}","page":"Functions","title":"OneTwoTree.verify_forest_args","text":"verify_forest_args(n_trees::Int, n_features_per_tree::Int, max_depth::Int)\n\nVerifies the validity of arguments used to initialize a forest.\n\nArguments:\n\nn_trees::Int: The number of trees in the forest.\nn_features_per_tree::Int: The number of randomly drawn features used for each tree.\nmax_depth::Int: The upper bound for the depth of each tree.\n\nErrors:\n\nRaises an error if any of the arguments are less than or equal to zero.\n\n\n\n\n\n","category":"method"},{"location":"getting_started/#Prerequisites","page":"Getting Started","title":"🛠️ Prerequisites","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Prerequisite Version Installation Guide Required\nJulia 1.10 (Image: Julia) ✅","category":"page"},{"location":"getting_started/#Getting-Started","page":"Getting Started","title":"🚀 Getting Started","text":"","category":"section"},{"location":"getting_started/#Downloading-the-Package","page":"Getting Started","title":"✨ Downloading the Package","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Via Pkg> mode (press ] in Julia REPL):","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"activate --temp\nadd https://github.com/nichtJakob/OneTwoTree.jl.git","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"For Pluto notebooks: We can't use Pluto's environments but have to create our own:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"using Pkg\nPkg.activate(\"MyEnvironment\")\nPkg.add(url=\"https://github.com/nichtJakob/OneTwoTree.jl.git\")\nusing OneTwoTree","category":"page"},{"location":"getting_started/#**Example**","page":"Getting Started","title":"▶️ Example","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Note that that the Tree Construction in its current state can be very slow. Therefore, it may be advised to use small training datasets for the moment.","category":"page"},{"location":"getting_started/#Classification","page":"Getting Started","title":"Classification","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"using OneTwoTree\ndataset = [ # The rows are the different data points\n3.5 9.1 2.9\n1.0 1.2 0.4\n5.6 3.3 4.3\n]\nlabels = [\"A\", \"B\", \"C\"]\n\ntree = DecisionTreeClassifier(max_depth=2)\nfit!(tree, dataset, labels) # train the tree with the data\nprint(tree)\n\nprediction = predict(tree, [\n2.0 4.0 6.0\n])\nprint(\"\\nThe tree predicted class $(prediction[1]).\")","category":"page"},{"location":"getting_started/#Regression","page":"Getting Started","title":"Regression","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"using OneTwoTree\ndataset = [\n  1.0 2.0\n  2.0 3.0\n  3.0 4.0\n  4.0 5.0\n]\nlabels = [1.5, 2.5, 3.5, 4.5]\n\ntree = DecisionTreeRegressor(max_depth=3)\nfit!(tree, dataset, labels)\nprint(tree)\n\nprediction = predict(tree, [\n  1.0 4.0\n])\nprint(\"\\nThe tree predicted $(prediction[1]).\")","category":"page"},{"location":"getting_started/#Forests-and-Loading-Other-Datasets","page":"Getting Started","title":"Forests and Loading Other Datasets","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"You can find more extensive examples utilising the Iris and BostonHousing datasets from MLDatasets in demo_classification.jl. and demo_regression.jl. The latter further compares DecisionTree performance to that of a Forest.","category":"page"},{"location":"getting_started/#Further-Reading-for-Developers","page":"Getting Started","title":"📚 Further Reading for Developers","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"✨ Downloading the Code for Local Development","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"git clone https://github.com/nichtJakob/OneTwoTree.jl.git","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"🔧 Installation and Dependency Setup\nRun the following commands in the package's root directory to install the dependencies and activate the package's virtual environment:\njulia --project\nIt might be necessary to resolve dependencies.\nGo into Pkg> mode by pressing ]. Then type     julia     resolve\nTo execute the tests, type in Pkg> mode:\ntest\nor in your julia REPL run:\ninclude(\"runtests.jl\")         # run all tests\ninclude(\"trees_tests/regression_tests.jl\") # run specific test (example)\nFor a quick guide on how to develop julia packages, write tests, ...,  read this.","category":"page"},{"location":"getting_started/#Project-Structure","page":"Getting Started","title":"🏙️ Project Structure","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Here is an overview of the project's main components:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"src/: Contains code for the main functionality of the package.\nOneTwoTree.jl: Main entry point of the project. Contains includes and exports.\nsplitting_criteria/: Splitting Criteria to construct trees from datasets.\ngini.jl: Gini Impurity\ninfo_gain: Information Gain\nvar_gain.jl: Variance Gain\ntrees/\ncart/: CART algorithm for constructing trees from data.\ntree.jl: Classification and regression trees.\nnode.jl: One node of a decision tree.\nforest.jl: Classification and regression forests for aggregating the decisions of multiple decision trees.\ndecision_function.jl: Decision of a node, e.g. is x[1] > 5 ?\nutils/: Functions we didn't want to import a dependency for.\ntest/: Unit tests to ensure correctness of various package components.\ndocs/: Files for the external documentation (which you are currently reading :D).","category":"page"},{"location":"demo_regression/","page":"Demo Regression","title":"Demo Regression","text":"In this example we want to compare the performance of regression trees with regression random forests on the BostonHousing dataset by using the OneTwoTree package.","category":"page"},{"location":"demo_regression/","page":"Demo Regression","title":"Demo Regression","text":"If you want to execute this example in your julia REPL you will first need to install some dependencies:","category":"page"},{"location":"demo_regression/","page":"Demo Regression","title":"Demo Regression","text":"julia>  using Pkg\njulia>  Pkg.add(\"MLDatasets\")\njulia>  Pkg.add(\"DataFrames\")\njulia>  Pkg.add(url=\"https://github.com/nichtJakob/OneTwoTree.jl.git\")","category":"page"},{"location":"demo_regression/","page":"Demo Regression","title":"Demo Regression","text":"Import the needed dependencies.","category":"page"},{"location":"demo_regression/","page":"Demo Regression","title":"Demo Regression","text":"using OneTwoTree\nusing MLDatasets: BostonHousing\nusing Random\nusing Statistics\nusing DataFrames","category":"page"},{"location":"demo_regression/","page":"Demo Regression","title":"Demo Regression","text":"First we load the data.","category":"page"},{"location":"demo_regression/","page":"Demo Regression","title":"Demo Regression","text":"dataset = BostonHousing(as_df=false)\nX, y = dataset[:]\nn_samples = size(X, 1)","category":"page"},{"location":"demo_regression/","page":"Demo Regression","title":"Demo Regression","text":"We split randomly into training and test sets. X are the features and y are the targets. You can experiment with different train ratios as long as it stays between 0.1 and 0.9.","category":"page"},{"location":"demo_regression/","page":"Demo Regression","title":"Demo Regression","text":"train_ratio = 0.8\nn_train = Int(round(train_ratio * n_samples))\nindices = randperm(n_samples)\ntrain_idx = indices[1:n_train]\ntest_idx = indices[n_train+1:end]\nX_train, y_train = X[train_idx, :], y[train_idx]\nX_test, y_test = X[test_idx, :], y[test_idx]","category":"page"},{"location":"demo_regression/","page":"Demo Regression","title":"Demo Regression","text":"Now we use the OneTwoTree package to plant a regression tree. You can experiment with different maximum tree depths.","category":"page"},{"location":"demo_regression/","page":"Demo Regression","title":"Demo Regression","text":"tree = DecisionTreeRegressor(max_depth=5)","category":"page"},{"location":"demo_regression/","page":"Demo Regression","title":"Demo Regression","text":"We train it on the training data.","category":"page"},{"location":"demo_regression/","page":"Demo Regression","title":"Demo Regression","text":"fit!(tree, X_train, y_train)\n\nprintln(\"\\n\\nOur Tree:\\n\")\nprint_tree(tree)","category":"page"},{"location":"demo_regression/","page":"Demo Regression","title":"Demo Regression","text":"Now let's look at regression forests. You can experiment with the parameters and see how the performance varies.","category":"page"},{"location":"demo_regression/","page":"Demo Regression","title":"Demo Regression","text":"forest = ForestRegressor(n_trees=5, n_features_per_tree=40, max_depth=30)\nfit!(forest, X_train, y_train)","category":"page"},{"location":"demo_regression/","page":"Demo Regression","title":"Demo Regression","text":"Let's admire our forest by printing it to the console If you have chosen a large number of trees, you might want to not execute the following two lines.","category":"page"},{"location":"demo_regression/","page":"Demo Regression","title":"Demo Regression","text":"println(\"\\n \\n Our forest: \\n\")\nprint_forest(forest)","category":"page"},{"location":"demo_regression/","page":"Demo Regression","title":"Demo Regression","text":"Let's check the tree performance on test data.","category":"page"},{"location":"demo_regression/","page":"Demo Regression","title":"Demo Regression","text":"y_pred_tree = predict(tree, X_test)\n\nmse_tree = mean((y_pred_tree .- y_test).^2)    #Mean Squared Error\nrmse_tree = sqrt(mse_tree)                    #Root Mean Squared Error\nmae_tree = mean(abs.(y_pred_tree .- y_test))    #Mean Absolute Error","category":"page"},{"location":"demo_regression/","page":"Demo Regression","title":"Demo Regression","text":"And now the forest performance on test data.","category":"page"},{"location":"demo_regression/","page":"Demo Regression","title":"Demo Regression","text":"y_pred_forest = predict(forest, X_test)\n\nmse_forest = mean((y_pred_forest .- y_test).^2)    #Mean Squared Error\nrmse_forest = sqrt(mse_forest)                  # Root Mean Squared Error\nmae_forest = mean(abs.(y_pred_forest .- y_test))  # Mean Absolute Error","category":"page"},{"location":"demo_regression/","page":"Demo Regression","title":"Demo Regression","text":"Finally, we can compare our regression tree with the random forest regressor.","category":"page"},{"location":"demo_regression/","page":"Demo Regression","title":"Demo Regression","text":"println(\"\\nTest Performance comparison:\")\nprintln(\"-------------------------------\\n\")\n\nprintln(\"Mean Squared Error (MSE)\")\nprintln(\"Tree   (MSE): $mse_tree\")\nprintln(\"Forest (MSE): $mse_forest\\n\")\n\nprintln(\"Root Mean Squared Error (RMSE)\")\nprintln(\"Tree   (RMSE): $rmse_tree\")\nprintln(\"Forest (RMSE): $rmse_forest\\n\")\n\nprintln(\"Mean Absolute Error (MAE)\")\nprintln(\"Tree   (MAE): $mae_tree\")\nprintln(\"Forest (MAE): $mae_forest\\n\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = OneTwoTree","category":"page"},{"location":"#Brief-Explanation","page":"Home","title":"Brief Explanation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Decision Trees are a supervised learning algorithm used for classification and regression tasks. They split the data into subsets based on feature values, forming a tree-like structure where each internal node represents a decision based on a feature, and each leaf node represents a predicted outcome.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Random Forests improve on Decision Trees by creating an ensemble of multiple decision trees, each trained on a random subset of the data. The final prediction is made by averaging the outputs of all trees (for regression) or using a majority vote (for classification), which helps reduce overfitting and improves model accuracy.","category":"page"},{"location":"#Index-for-Documentation","page":"Home","title":"Index for Documentation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for OneTwoTree.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"}]
}
