var documenterSearchIndex = {"docs":
[{"location":"","page":"Home","title":"Home","text":"CurrentModule = OneTwoTree","category":"page"},{"location":"#OneTwoTree","page":"Home","title":"OneTwoTree","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for OneTwoTree.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [OneTwoTree]","category":"page"},{"location":"#OneTwoTree.DecisionTreeClassifier","page":"Home","title":"OneTwoTree.DecisionTreeClassifier","text":"DecisionTreeClassifier\n\nA DecisionTreeClassifier is a tree of decision nodes. It can predict classes based on the input data. In addition to a root node it holds meta informations such as max_depth etc. Use fit(tree, features, labels) to create a tree from data\n\nArguments\n\nroot::Union{Node, Nothing}: the root node of the decision tree; nothing if the tree is empty\nmax_depth::Int: maximum depth of the decision tree; no limit if equal to -1\n\n\n\n\n\n","category":"type"},{"location":"#OneTwoTree.DecisionTreeClassifier-Tuple{}","page":"Home","title":"OneTwoTree.DecisionTreeClassifier","text":"Initialises a decision tree model.\n\nArguments\n\nroot::Union{Node, Nothing}: the root node of the decision tree; nothing if the tree is empty\nmax_depth::Int: maximum depth of the decision tree; no limit if equal to -1\n\n\n\n\n\n","category":"method"},{"location":"#OneTwoTree.DecisionTreeRegressor","page":"Home","title":"OneTwoTree.DecisionTreeRegressor","text":"DecisionTreeRegressor\n\nA DecisionTreeRegressor is a tree of decision nodes. It can predict function values based on the input data. In addition to a root node it holds meta informations such as max_depth etc. Use fit(tree, features, labels) to create a tree from data\n\nArguments\n\nroot::Union{Node, Nothing}: the root node of the decision tree; nothing if the tree is empty\nmax_depth::Int: maximum depth of the decision tree; no limit if equal to -1\n\n\n\n\n\n","category":"type"},{"location":"#OneTwoTree.DecisionTreeRegressor-Tuple{}","page":"Home","title":"OneTwoTree.DecisionTreeRegressor","text":"Initialises a decision tree model.\n\nArguments\n\nroot::Union{Node, Nothing}: the root node of the decision tree; nothing if the tree is empty\nmax_depth::Int: maximum depth of the decision tree; no limit if equal to -1\n\n\n\n\n\n","category":"method"},{"location":"#OneTwoTree.Node","page":"Home","title":"OneTwoTree.Node","text":"Node\n\nA Node represents a decision in the Tree. It is a leaf with a prediction or has exactly one true and one false child and a decision function.\n\n\n\n\n\n","category":"type"},{"location":"#OneTwoTree.build_tree-Union{Tuple{T}, Tuple{S}, Tuple{Matrix{S}, Vector{T}, Int64}} where {S<:Union{Real, String}, T<:Union{Real, String}}","page":"Home","title":"OneTwoTree.build_tree","text":"build_tree(features, labels, max_depth, ...)\n\nBuilds a decision tree from the given data using some algorithm (e.g. CART)\n\nArguments\n\ntree::AbstractDecisionTree: the tree to be trained\ndataset::Matrix{Union{Real, String}}: the training data\nlabels::Vector{Union{Real, String}}: the target labels\nmax_depth::Int: the maximum depth of the created tree\ncolumn_data::Bool: whether the datapoints are contained in dataset columnwise\n\n\n\n\n\n","category":"method"},{"location":"#OneTwoTree.calc_accuracy-Tuple{Any, Any}","page":"Home","title":"OneTwoTree.calc_accuracy","text":"calc_accuracy(labels, predictions)\n\nCalculates the accuracy of the predictions compared to the labels.\n\n\n\n\n\n","category":"method"},{"location":"#OneTwoTree.calc_depth-Tuple{AbstractDecisionTree}","page":"Home","title":"OneTwoTree.calc_depth","text":"depth(tree)\n\nTraverses the tree and returns the maximum depth.\n\n\n\n\n\n","category":"method"},{"location":"#OneTwoTree.collect_classes-Union{Tuple{S}, Tuple{Matrix{S}, Int64}} where S<:Union{Real, String}","page":"Home","title":"OneTwoTree.collect_classes","text":"collect_classes(dataset, column)\n\nCollect all unique classes among the specified column of the dataset.\n\nArguments\n\ndataset::Matrix{Union{Real, String}}: the dataset to collect classes on\ncolumn::Int64: the index of the dataset column/feature to collect the classes on\n\n\n\n\n\n","category":"method"},{"location":"#OneTwoTree.equal-Tuple{Any, String}","page":"Home","title":"OneTwoTree.equal","text":"equal\n\nA basic categorical decision function for testing and playing around.\n\n\n\n\n\n","category":"method"},{"location":"#OneTwoTree.fit!-Union{Tuple{T}, Tuple{S}, Tuple{AbstractDecisionTree, Matrix{S}, Vector{T}}, Tuple{AbstractDecisionTree, Matrix{S}, Vector{T}, Any}} where {S<:Union{Real, String}, T<:Union{Number, String}}","page":"Home","title":"OneTwoTree.fit!","text":"fit!(tree, features, labels)\n\nTrain a decision tree on the given data using some algorithm (e.g. CART).\n\nArguments\n\ntree::AbstractDecisionTree: the tree to be trained\ndataset::Matrix{Union{Real, String}}: the training data\nlabels::Vector{Union{Real, String}}: the target labels\ncolumn_data::Bool: whether the datapoints are contained in dataset columnwise\n\n\n\n\n\n","category":"method"},{"location":"#OneTwoTree.is_leaf-Tuple{Node}","page":"Home","title":"OneTwoTree.is_leaf","text":"is_leaf(node)\n\nDo you seriously expect a description for this?\n\n\n\n\n\n","category":"method"},{"location":"#OneTwoTree.label_mean-Union{Tuple{T}, Tuple{Vector{T}, Vector{Int64}}} where T<:Real","page":"Home","title":"OneTwoTree.label_mean","text":"label_mean(labels, indices)\n\nCalculate the mean of a subset of numeric labels.\n\nlabels::Vector{Real}: the vector of numeric labels\nindices::Vector{Int64}: the indices of the numeric labels to be considered\n\n\n\n\n\n","category":"method"},{"location":"#OneTwoTree.label_mean-Union{Tuple{Vector{T}}, Tuple{T}} where T<:Real","page":"Home","title":"OneTwoTree.label_mean","text":"label_mean(labels, indices)\n\nCalculate the mean of numeric labels.\n\nArguments\n\nlabels::Vector{Real}: the vector of numeric labels\n\n\n\n\n\n","category":"method"},{"location":"#OneTwoTree.lessThanOrEqual-Tuple{Any, Float64}","page":"Home","title":"OneTwoTree.lessThanOrEqual","text":"lessThanOrEqual\n\nA basic numerical decision function for testing and playing around.\n\n\n\n\n\n","category":"method"},{"location":"#OneTwoTree.load_data-Tuple{Any}","page":"Home","title":"OneTwoTree.load_data","text":"load_data(name::String)\n\nLoad a preconfigured dataset from a CSV file.\n\nArguments\n\nname::String: the name of the dataset to load\n\nExample\n\nload_data(\"fashion_mnist_1000\")\n\n\n\n\n\n","category":"method"},{"location":"#OneTwoTree.most_frequent_class-Union{Tuple{T}, Tuple{Vector{T}, Vector{Int64}}} where T<:Union{Int64, String}","page":"Home","title":"OneTwoTree.most_frequent_class","text":"most_frequent_class(labels, indices)\n\nDetermine the most frequent class among a subset of class labels.\n\nArguments\n\nlabels::Vector{Union{Int, String}}: class labels\nindices::Vector{Int64}: the indices of the class labels, to be considered/counted\n\n\n\n\n\n","category":"method"},{"location":"#OneTwoTree.predict-Union{Tuple{S}, Tuple{AbstractDecisionTree, VecOrMat{S}}} where S<:Union{Real, String}","page":"Home","title":"OneTwoTree.predict","text":"predict\n\nTraverses the tree for a given datapoint x and returns that trees prediction.\n\nArguments\n\ntree::AbstractDecisionTree: the tree to predict with\nX::Union{Matrix{S}, Vector{S}: the data to predict on\n\n\n\n\n\n","category":"method"},{"location":"#OneTwoTree.printM-Union{Tuple{Matrix{S}}, Tuple{S}} where S<:Union{Real, String}","page":"Home","title":"OneTwoTree.printM","text":"printM(M)\n\nPrint the matrix M:\n\nArguments\n\nM::Matrix{Union{Real, String}}\n\n\n\n\n\n","category":"method"},{"location":"#OneTwoTree.print_tree-Tuple{AbstractDecisionTree}","page":"Home","title":"OneTwoTree.print_tree","text":"print_tree(tree::AbstractDecisionTree)\n\nPrints a textual visualization of the decision tree. For each decision node, it displays the condition, and for each leaf, it displays the prediction.\n\nArguments\n\ntree::AbstractDecisionTree The DecisionTree instance to print.\n\nExample output:\n\nx < 28 ? ├─ False: y < 161 ?    ├─ False: 842    └─ True: 2493 └─ True: 683\n\n\n\n\n\n","category":"method"},{"location":"#OneTwoTree.split_indices-Union{Tuple{S}, Tuple{Matrix{S}, Vector{Int64}, Function}} where S<:Union{Real, String}","page":"Home","title":"OneTwoTree.split_indices","text":"split_indices(dataset, node_data, decision_fn)\n\nSplit the dataset indices contained in node_data into two sets via the decision function.\n\nArguments\n\ndataset::Matrix{Union{Real, String}}: the dataset to split on (the datapoints are assumed to be contained rowwise)\nnode_data::Vector{Int64}: the index list, indexing the dataset, to be split\ndecision_fn::Function: the decision function taking as input a datapoint and returning a Bool\n\n\n\n\n\n","category":"method"},{"location":"#OneTwoTree.split_indices-Union{Tuple{T}, Tuple{S}, Tuple{Matrix{S}, Vector{Int64}, Function, T, Int64}} where {S<:Union{Real, String}, T<:Union{Real, String}}","page":"Home","title":"OneTwoTree.split_indices","text":"split_indices(dataset, node_data, decision_fn, decision_param, decision_feature)\n\nSplit the dataset indices contained in node_data into two sets via the decision function.\n\nArguments\n\ndataset::Matrix{Union{Real, String}}: the dataset to split on (the datapoints are assumed to be contained rowwise)\nnode_data::Vector{Int64}: the index list, indexing dataset, to be split\ndecision_fn::Function: the decision function taking as input a datapoint, decisionparam and decisionfeature and returning a Bool\ndecision_param::Union{Real, String}: a parameter to the decision function. This can be a class name or a numeric threshold.\ndecision_feature::Int64: the index of the dimension of datapoints along which to split\n\n\n\n\n\n","category":"method"}]
}
