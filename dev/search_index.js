var documenterSearchIndex = {"docs":
[{"location":"functions/","page":"Functions","title":"Functions","text":"Modules = [OneTwoTree]","category":"page"},{"location":"functions/#OneTwoTree.AbstractForest","page":"Functions","title":"OneTwoTree.AbstractForest","text":"Base type for classification and regression forests. Forests are collections of trees which aggregate their decisions.  \n\n\n\n\n\n","category":"type"},{"location":"functions/#OneTwoTree.Decision","page":"Functions","title":"OneTwoTree.Decision","text":"Decision\n\nA structure representing a decision with a function and its parameter.\n\nParameters\n\nfn::Function: The decision function.\nparam::Union{Number, String}: The parameter for the decision function.\nNumber: for comparison functions (e.g. x < 5.0)\nString: for True/False functions (e.g. x == \"red\" or x != 681)\nfeature::Int64: The index of the feature to compare.\n\n\n\n\n\n","category":"type"},{"location":"functions/#OneTwoTree.DecisionTreeClassifier","page":"Functions","title":"OneTwoTree.DecisionTreeClassifier","text":"DecisionTreeClassifier\n\nA DecisionTreeClassifier is a tree of decision nodes. It can predict classes based on the input data. In addition to a root node it holds meta informations such as max_depth etc. Use fit(tree, features, labels) to create a tree from data\n\nArguments\n\nroot::Union{Node, Nothing}: the root node of the decision tree; nothing if the tree is empty\nmax_depth::Int: maximum depth of the decision tree; no limit if equal to -1\n\n\n\n\n\n","category":"type"},{"location":"functions/#OneTwoTree.DecisionTreeClassifier-Tuple{}","page":"Functions","title":"OneTwoTree.DecisionTreeClassifier","text":"Initialises a decision tree model.\n\nArguments\n\nroot::Union{Node, Nothing}: the root node of the decision tree; nothing if the tree is empty\nmax_depth::Int: maximum depth of the decision tree; no limit if equal to -1\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.DecisionTreeRegressor","page":"Functions","title":"OneTwoTree.DecisionTreeRegressor","text":"DecisionTreeRegressor\n\nA DecisionTreeRegressor is a tree of decision nodes. It can predict function values based on the input data. In addition to a root node it holds meta informations such as max_depth etc. Use fit(tree, features, labels) to create a tree from data\n\nArguments\n\nroot::Union{Node, Nothing}: the root node of the decision tree; nothing if the tree is empty\nmax_depth::Int: maximum depth of the decision tree; no limit if equal to -1\n\n\n\n\n\n","category":"type"},{"location":"functions/#OneTwoTree.DecisionTreeRegressor-Tuple{}","page":"Functions","title":"OneTwoTree.DecisionTreeRegressor","text":"Initialises a decision tree model.\n\nArguments\n\nroot::Union{Node, Nothing}: the root node of the decision tree; nothing if the tree is empty\nmax_depth::Int: maximum depth of the decision tree; no limit if equal to -1\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.ForestClassifier","page":"Functions","title":"OneTwoTree.ForestClassifier","text":"Forest for classification problems\n\nFields:\n\ntrees::Vector{DecisionTreeClassifier}: A vector of decision trees.\nn_trees::Int: The number of trees in the forest.\nn_features_per_tree::Int: The number of randomly drawn features used for each tree.\nmax_depth::Int: The upper bound for the depth of each tree.\n\n\n\n\n\n","category":"type"},{"location":"functions/#OneTwoTree.ForestClassifier-Tuple{}","page":"Functions","title":"OneTwoTree.ForestClassifier","text":"Constructs a ForestClassifier instance.\n\nArguments:\n\nn_trees::Int: The number of trees in the forest.\nn_features_per_tree::Int: The number of randomly drawn features used for each tree.\nmax_depth::Int: The upper bound for the depth of each tree.\n\nReturns:\n\nA ForestClassifier instance.\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.ForestRegressor","page":"Functions","title":"OneTwoTree.ForestRegressor","text":"Forest for regression problems\n\nFields:\n\ntrees::Vector{DecisionTreeRegressor}: A vector of regression trees.\nn_trees::Int: The number of trees in the forest.\nn_features_per_tree::Int: The number of randomly drawn features used for each tree.\nmax_depth::Int: The upper bound for the depth of each tree.\n\n\n\n\n\n","category":"type"},{"location":"functions/#OneTwoTree.ForestRegressor-Tuple{}","page":"Functions","title":"OneTwoTree.ForestRegressor","text":"Constructs a ForestRegressor instance.\n\nArguments:\n\nn_trees::Int: The number of trees in the forest.\nn_features_per_tree::Int: The number of randomly drawn features used for each tree.\nmax_depth::Int: The upper bound for the depth of each tree.\n\nReturns:\n\nA ForestRegressor instance.\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.Node","page":"Functions","title":"OneTwoTree.Node","text":"Node\n\nA Node represents a decision in the Tree. It is a leaf with a prediction or has exactly one true and one false child and a decision function.\n\n\n\n\n\n","category":"type"},{"location":"functions/#Base.show-Tuple{IO, OneTwoTree.AbstractForest}","page":"Functions","title":"Base.show","text":"Displays a string representation of the forest when shown in the REPL or other I/O streams.\n\nArguments:\n\nio::IO: The I/O stream.\nforest::AbstractForest: The forest to be displayed.\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree._decision_to_string-Tuple{OneTwoTree.Decision}","page":"Functions","title":"OneTwoTree._decision_to_string","text":"_decision_to_string(d::DecisionFn)\n\nReturns a string representation of the decision function.\n\nArguments\n\nd::DecisionFn: The decision function to convert to a string.\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree._forest_to_string-Tuple{OneTwoTree.AbstractForest}","page":"Functions","title":"OneTwoTree._forest_to_string","text":"Converts the forest to a string.\n\nArguments:\n\nforest::AbstractForest: The forest to be represented.\n\nReturns:\n\nA string with numerated text representations of the trees in the forest\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree._node_to_string-Tuple{OneTwoTree.Node, Bool, String}","page":"Functions","title":"OneTwoTree._node_to_string","text":"_node_to_string(node::Node, prefix::String, is_true_child::Bool, indentation::String)\n\nRecursive helper function to stringify the decision tree structure.\n\nArguments\n\nnode: The current node to print.\nis_true_child: Boolean indicating if the node is a true branch child.\nindentation: The current indentation.\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree._tree_to_string","page":"Functions","title":"OneTwoTree._tree_to_string","text":"tree_to_string(tree::AbstractDecisionTree)\n\nReturns a textual visualization of the decision tree.\n\nArguments\n\ntree::AbstractDecisionTree The DecisionTree instance to print.\n\nExample output:\n\nx < 28.0 ? ‚îú‚îÄ False: x == 161.0 ? ‚îÇ  ‚îú‚îÄ False: 842 ‚îÇ  ‚îî‚îÄ True: 2493 ‚îî‚îÄ True: 683\n\n\n\n\n\n","category":"function"},{"location":"functions/#OneTwoTree._verify_fit!_args-NTuple{4, Any}","page":"Functions","title":"OneTwoTree._verify_fit!_args","text":"Some guards to ensure the input data is valid for training a tree.\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.calc_accuracy-Union{Tuple{T}, Tuple{S}, Tuple{AbstractArray{S}, AbstractArray{T}}} where {S, T}","page":"Functions","title":"OneTwoTree.calc_accuracy","text":"calc_accuracy(labels, predictions)\n\nCalculates the accuracy of the predictions compared to the labels.\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.calc_depth-Tuple{OneTwoTree.AbstractDecisionTree}","page":"Functions","title":"OneTwoTree.calc_depth","text":"depth(tree)\n\nTraverses the tree and returns the maximum depth.\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.collect_classes-Tuple{AbstractMatrix, Int64}","page":"Functions","title":"OneTwoTree.collect_classes","text":"collect_classes(dataset, column)\n\nCollect all unique classes among the specified column of the dataset.\n\nArguments\n\ndataset::AbstractMatrix: the dataset to collect classes on\ncolumn::Int64: the index of the dataset column/feature to collect the classes on\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.collect_classes-Tuple{AbstractMatrix, Vector{Int64}, Int64}","page":"Functions","title":"OneTwoTree.collect_classes","text":"collect_classes(dataset, column)\n\nCollect all unique classes among a subset of the specified column of the dataset.\n\nArguments\n\ndataset::AbstractMatrix: the dataset to collect classes on\nindices::Vector{Int64}: the indices of the numeric labels to be considered\ncolumn::Int64: the index of the dataset column/feature to collect the classes on\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.entropy-Tuple{Any}","page":"Functions","title":"OneTwoTree.entropy","text":"entropy(features::AbstractVector) -> Float64\n\nThis function calculates the entropy of set of lables\n\nArguments:\n\nlabels: A vector of labels\n\nReturns:\n\nThe entropy H(X) = -sum^n{i=1} P(xi) log2(P(xi))   with X beeing the labels   and n the number of elements in X   and P() beeing the Probability \n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.equal-Tuple{Any, String}","page":"Functions","title":"OneTwoTree.equal","text":"equal\n\nA basic categorical decision function for testing and playing around.\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.fit!-Union{Tuple{T}, Tuple{OneTwoTree.AbstractDecisionTree, AbstractMatrix, Vector{T}}} where T<:Union{Number, String}","page":"Functions","title":"OneTwoTree.fit!","text":"fit!(tree, features, labels)\n\nTrain a decision tree on the given data using some algorithm (e.g. CART).\n\nArguments\n\ntree::AbstractDecisionTree: the tree to be trained\ndataset::AbstractMatrix: the training data\nlabels::Vector{Union{Real, String}}: the target labels\n`splitting_criterion: a function indicating some notion of gain from splitting a node.\ncolumn_data::Bool: whether the datapoints are contained in dataset columnwise\n\n(OneTwoTree provides the following splitting criteria for classification: ginigain, informationgain; and for regression: variance_gain. If you'd like to define a splitting criterion yourself, you need to consider the following:\n\nThe function must calculate a 'gain'-value for a split of a node, meaning that larger values are considered better.\nThe function signature must conform to my_func(features::AbstractMatrix, labels::AbstractVector, node_data::Vector{Int64}, decision_fn::Function, decision_param::Union{Real, String}, decision_feature::Int64)) where features is the total data matrix, labels the total label vector and node_data is an index array, that indexes into the former to define a subset of the data to be considered. The last three values define a split of the data.\nYou can get the split labelsets for your computation by taking analogous steps as in the OneTwoTree.split_indices method.)\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.fit!-Union{Tuple{T}, Tuple{S}, Tuple{OneTwoTree.AbstractForest, Matrix{S}, Vector{T}}} where {S<:Union{Real, String}, T<:Union{Number, String}}","page":"Functions","title":"OneTwoTree.fit!","text":"Trains the forest model on a given trainingdataset consisting of training features and corresponding labels.\n\nArguments:\n\nforest::AbstractForest: The forest to be trained.\nfeatures::Matrix{S}: The training feature matrix.\nlabels::Vector{T}: The training labels vector.\ncolumn_data::Bool: Whether the features are column-oriented (default is false).\n\nBehavior:\n\nTrains each tree in the forest on randomly drawn subsets of test features and corresponding test labels.\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.get_random_features-Union{Tuple{T}, Tuple{S}, Tuple{Matrix{S}, Vector{T}, Int64}} where {S<:Union{Real, String}, T<:Union{Number, String}}","page":"Functions","title":"OneTwoTree.get_random_features","text":"Returns random features and their corresponding labels from the given dataset.\n\nArguments:\n\nfeatures::Matrix{S}: The feature matrix.\nlabels::Vector{T}: The labels vector.\nn_features::Int: The number of features to draw randomly.\n\nReturns:\n\nA tuple (random_features, random_labels) containing the randomly drawn features and labels.\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.gini_gain-Tuple{AbstractVector, AbstractVector, AbstractVector}","page":"Functions","title":"OneTwoTree.gini_gain","text":"gini_gain(parent_labels::AbstractVector, true_child_labels::AbstractVector, false_child_labels) -> Float64\n\nThis function calculates the gain in Gini impurity for a split in a decision tree. The split is characterized by the partition of the parentlabels into truechildlabels and falsechild_labels according to some discriminant function.\n\nArguments:\n\nparent_labels: A vector of data labels (e.g., classes or numerical values in the case of regression).\ntrue_child_labels: A vector of a subset of data labels contained in parent_labels.\nfalse_child_labels: A vector of a subset of data labels contained in parent_labels.\n\nReturns:\n\nThe gain in Gini impurity of the split.\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.information_gain-Tuple{AbstractVector, AbstractVector, AbstractVector}","page":"Functions","title":"OneTwoTree.information_gain","text":"information_gain(parent_labels::AbstractVector, true_child_labels::AbstractVector, false_child_labels) -> Float64\n\nThis function calculates the information gain for a split in a decision tree. The split is characterized by the partition of the parentlabels into truechildlabels and falsechild_labels according to some discriminant function.\n\nArguments:\n\nparent_labels: A vector of data labels (e.g., classes or numerical values in the case of regression).\ntrue_child_labels: A vector of a subset of data labels contained in parent_labels.\nfalse_child_labels: A vector of a subset of data labels contained in parent_labels.\n\nReturns:\n\nThe information gain of the split.\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.is_leaf-Tuple{OneTwoTree.Node}","page":"Functions","title":"OneTwoTree.is_leaf","text":"is_leaf(node)\n\nDo you seriously expect a description for this?\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.label_mean-Union{Tuple{T}, Tuple{Vector{T}, Vector{Int64}}} where T<:Real","page":"Functions","title":"OneTwoTree.label_mean","text":"label_mean(labels, indices)\n\nCalculate the mean of a subset of numeric labels.\n\nlabels::Vector{Real}: the vector of numeric labels\nindices::Vector{Int64}: the indices of the numeric labels to be considered\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.less_than_or_equal-Tuple{Any, Float64}","page":"Functions","title":"OneTwoTree.less_than_or_equal","text":"less_than_or_equal\n\nA basic numerical decision function for testing and playing around.\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.most_frequent_class-Union{Tuple{T}, Tuple{Vector{T}, Vector{Int64}}} where T<:Union{Int64, String}","page":"Functions","title":"OneTwoTree.most_frequent_class","text":"most_frequent_class(labels, indices)\n\nDetermine the most frequent class among a subset of class labels.\n\nArguments\n\nlabels::Vector{Union{Int, String}}: class labels\nindices::Vector{Int64}: the indices of the class labels, to be considered/counted\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.predict-Tuple{OneTwoTree.AbstractDecisionTree, Union{AbstractMatrix, AbstractVector}}","page":"Functions","title":"OneTwoTree.predict","text":"predict\n\nTraverses the tree for a given datapoint x and returns that trees prediction.\n\nArguments\n\ntree::AbstractDecisionTree: the tree to predict with\nX::Union{AbstractMatrix, AbstractVector: the data to predict on\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.predict-Union{Tuple{S}, Tuple{OneTwoTree.AbstractForest, VecOrMat{S}}} where S<:Union{Real, String}","page":"Functions","title":"OneTwoTree.predict","text":"Outputs the forest-prediction for a given datapoint X.  The prediction is based on the aggregation of the tree decisions. For agregation in a regression scenario the mean is used. For agregation in a classification scenario the most voted class label is used.\n\nArguments:\n\nforest::AbstractForest: The trained forest.\nX::Union{Matrix{S}, Vector{S}}: The input data for which a prediction is searched.\n\nReturns:\n\nPredictions for the input data X, aggregated across all trees in the forest.\n\nErrors:\n\nRaises an error if the forest contains no trained trees.\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.printM-Union{Tuple{Matrix{S}}, Tuple{S}} where S<:Union{Real, String}","page":"Functions","title":"OneTwoTree.printM","text":"printM(M)\n\nPrint the matrix M:\n\nArguments\n\nM::Matrix{Union{Real, String}}\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.print_forest-Tuple{OneTwoTree.AbstractForest}","page":"Functions","title":"OneTwoTree.print_forest","text":"Prints the numerated trees of a forest.\n\nArguments:\n\nforest::AbstractForest: The forest to be printed.\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.print_tree-Tuple{OneTwoTree.AbstractDecisionTree}","page":"Functions","title":"OneTwoTree.print_tree","text":"print_tree(tree::AbstractDecisionTree)\n\nReturns a textual visualization of the decision tree.\n\nArguments\n\ntree::AbstractDecisionTree The DecisionTree instance to print.\n\nExample output:\n\nx < 28.0 ? ‚îú‚îÄ False: x == 161.0 ? ‚îÇ  ‚îú‚îÄ False: 842 ‚îÇ  ‚îî‚îÄ True: 2493 ‚îî‚îÄ True: 683\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.should_split-Tuple{OneTwoTree.Node, Float64, Int64}","page":"Functions","title":"OneTwoTree.should_split","text":"should_split(N, splitting_gain, max_depth)\n\nDetermines whether to split the node N given.\n\nArguments\n\nN::Node: Node that may be split. N contains further fields relevant to the decision like the best splitting decision functtion and maximum tree depth.\nsplitting_gain::Float64: The gain in the splitting criterion metric of N after it's optimal split.\nmax_depth::Int64: The maximum depth of the tree N is part of.\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.split-Tuple{OneTwoTree.Node, Function}","page":"Functions","title":"OneTwoTree.split","text":"split(N)\n\nDetermine the optimal split of a node. Handles numerical and categorical data and labels.\n\nArguments\n\nN::Node: The node to be split. All additional information for the split calculation (e.g. dataset, labels, node_data) is contained in N.\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.split_indices-Tuple{AbstractMatrix, Vector{Int64}, Function}","page":"Functions","title":"OneTwoTree.split_indices","text":"split_indices(dataset, node_data, decision_fn)\n\nSplit the dataset indices contained in node_data into two sets via the decision function.\n\nArguments\n\ndataset::AbstractMatrix: the dataset to split on (the datapoints are assumed to be contained rowwise)\nnode_data::Vector{Int64}: the index list, indexing the dataset, to be split\ndecision_fn::Function: the decision function taking as input a datapoint and returning a Bool\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.split_indices-Union{Tuple{T}, Tuple{AbstractMatrix, Vector{Int64}, Function, T, Int64}} where T<:Union{Real, String}","page":"Functions","title":"OneTwoTree.split_indices","text":"split_indices(dataset, node_data, decision_fn, decision_param, decision_feature)\n\nSplit the dataset indices contained in node_data into two sets via the decision function.\n\nArguments\n\ndataset::AbstractMatrix: the dataset to split on (the datapoints are assumed to be contained rowwise)\nnode_data::Vector{Int64}: the index list, indexing dataset, to be split\ndecision_fn::Function: the decision function taking as input a datapoint, decisionparam and decisionfeature and returning a Bool\ndecision_param::Union{Real, String}: a parameter to the decision function. This can be a class name or a numeric threshold.\ndecision_feature::Int64: the index of the dimension of datapoints along which to split\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.variance_gain-Tuple{AbstractVector, AbstractVector, AbstractVector}","page":"Functions","title":"OneTwoTree.variance_gain","text":"variance_gain(parent_labels::AbstractVector, true_child_labels::AbstractVector, false_child_labels) -> Float64\n\nThis function calculates the variance gain for a split in a decision tree. The split is characterized by the partition of the parentlabels into truechildlabels and falsechild_labels according to some discriminant function.\n\nArguments:\n\nparent_labels: A vector of data labels (e.g., classes or numerical values in the case of regression).\ntrue_child_labels: A vector of a subset of data labels contained in parent_labels.\nfalse_child_labels: A vector of a subset of data labels contained in parent_labels.\n\nReturns:\n\nThe variance gain of the split.\n\n\n\n\n\n","category":"method"},{"location":"functions/#OneTwoTree.verify_forest_args-Tuple{Int64, Int64, Int64}","page":"Functions","title":"OneTwoTree.verify_forest_args","text":"Verifies the validity of arguments used to initialize a forest.\n\nArguments:\n\nn_trees::Int: The number of trees in the forest.\nn_features_per_tree::Int: The number of randomly drawn features used for each tree.\nmax_depth::Int: The upper bound for the depth of each tree.\n\nErrors:\n\nRaises an error if any of the arguments are less than or equal to zero.\n\n\n\n\n\n","category":"method"},{"location":"getting_started/#Getting-Started","page":"Getting Started","title":"üöÄ Getting Started","text":"","category":"section"},{"location":"getting_started/#Downloading-the-Package","page":"Getting Started","title":"‚ú® Downloading the Package","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Via Pkg> mode (press ] in Julia REPL):","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"activate --temp\nadd https://github.com/nichtJakob/OneTwoTree.jl.git","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"For Pluto notebooks: We can't use Pluto's environments but have to create our own:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"using Pkg\nPkg.activate(\"MyEnvironment\")\nPkg.add(url=\"https://github.com/nichtJakob/OneTwoTree.jl.git\")\nusing OneTwoTree","category":"page"},{"location":"getting_started/#**Example:-Running-a-Simple-Example**","page":"Getting Started","title":"‚ñ∂Ô∏è Example: Running a Simple Example","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Note that that the Tree Construction in its current state can be very slow. Therefore, it may be advised to use small training datasets for the moment.","category":"page"},{"location":"getting_started/#Classification","page":"Getting Started","title":"Classification","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"using OneTwoTree\ndataset = [ # The rows are the different data points\n3.5 9.1 2.9\n1.0 1.2 0.4\n5.6 3.3 4.3\n]\nlabels = [\"A\", \"B\", \"C\"]\n\ntree = DecisionTreeClassifier(max_depth=2)\nfit!(tree, dataset, labels) # train the tree with the data\nprint(tree)\n\nprediction = predict(tree, [\n2.0 4.0 6.0\n])\nprint(\"The tree predicted class \\$(prediction[1]).\")","category":"page"},{"location":"getting_started/#Regression","page":"Getting Started","title":"Regression","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"using OneTwoTree\ndataset = [\n  1.0 2.0\n  2.0 3.0\n  3.0 4.0\n  4.0 5.0\n]\nlabels = [1.5, 2.5, 3.5, 4.5]\n\ntree = DecisionTreeRegressor(max_depth=3)\nfit!(tree, dataset, labels)\nprint(tree)\n\nprediction = predict(tree, [\n  1.0 4.0\n])\nprint(\"The tree predicted \\$(prediction[1]).\")","category":"page"},{"location":"getting_started/#Forests-and-Loading-Other-Datasets","page":"Getting Started","title":"Forests and Loading Other Datasets","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"You can find more extensive examples utilising the Iris and BostonHousing datasets from MLDatasets in demo_classification.jl. and demo_regression.jl. The latter further compares DecisionTree performance to that of a Forest.","category":"page"},{"location":"getting_started/#**Further-Reading-for-Developers**","page":"Getting Started","title":"üìö Further Reading for Developers","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"‚ú® Downloading the Code for Local Development\nbash    git clone https://github.com/nichtJakob/OneTwoTree.jl.git","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"üîß Installation and Dependency Setup\nRun the following commands in the package's root directory to install the dependencies and activate the package's virtual environment:\njulia --project\nIt might be necessary to resolve dependencies.\nGo into Pkg> mode by pressing ]. Then type    julia    resolve\nTo execute the tests, type in Pkg> mode:\ntest\nor in your julia REPL run:\ninclude(\"runtests.jl\")         # run all tests\ninclude(\"trees_tests/regression_tests.jl\") # run specific test (example)\nFor a quick guide on how to develop julia packages, write tests, ...,  read this.","category":"page"},{"location":"getting_started/#Contributors","page":"Getting Started","title":"üë©‚Äçüíª Contributors","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"(Image: Contributors)","category":"page"},{"location":"getting_started/#OneTwoTree","page":"Getting Started","title":"OneTwoTree","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = OneTwoTree","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: Dev) (Image: Build Status) (Image: Coverage)","category":"page"},{"location":"#Brief-Explanation","page":"Home","title":"Brief Explanation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Decision Trees are a supervised learning algorithm used for classification and regression tasks. They split the data into subsets based on feature values, forming a tree-like structure where each internal node represents a decision based on a feature, and each leaf node represents a predicted outcome.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Random Forests improve on Decision Trees by creating an ensemble of multiple decision trees, each trained on a random subset of the data. The final prediction is made by averaging the outputs of all trees (for regression) or using a majority vote (for classification), which helps reduce overfitting and improves model accuracy.","category":"page"},{"location":"#Prerequisites","page":"Home","title":"üõ†Ô∏è Prerequisites","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Prerequisite Version Installation Guide Required\nJulia 1.10 (Image: Julia) ‚úÖ","category":"page"},{"location":"#Index-for-Documentation","page":"Home","title":"Index for Documentation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for OneTwoTree.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"}]
}
