var documenterSearchIndex = {"docs":
[{"location":"","page":"Home","title":"Home","text":"CurrentModule = OneTwoTree","category":"page"},{"location":"#OneTwoTree","page":"Home","title":"OneTwoTree","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for OneTwoTree.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [OneTwoTree]","category":"page"},{"location":"#OneTwoTree.Decision","page":"Home","title":"OneTwoTree.Decision","text":"Decision\n\nA structure representing a decision with a function and its parameter.\n\nParameters\n\nfn::Function: The decision function.\nparam::Union{Number, String}: The parameter for the decision function.\nNumber: for comparison functions (e.g. x < 5.0)\nString: for True/False functions (e.g. x == \"red\" or x != 681)\nfeature::Int64: The index of the feature to compare.\n\n\n\n\n\n","category":"type"},{"location":"#OneTwoTree.DecisionTreeClassifier","page":"Home","title":"OneTwoTree.DecisionTreeClassifier","text":"DecisionTreeClassifier\n\nA DecisionTreeClassifier is a tree of decision nodes. It can predict classes based on the input data. In addition to a root node it holds meta informations such as max_depth etc. Use fit(tree, features, labels) to create a tree from data\n\nArguments\n\nroot::Union{Node, Nothing}: the root node of the decision tree; nothing if the tree is empty\nmax_depth::Int: maximum depth of the decision tree; no limit if equal to -1\n\n\n\n\n\n","category":"type"},{"location":"#OneTwoTree.DecisionTreeClassifier-Tuple{}","page":"Home","title":"OneTwoTree.DecisionTreeClassifier","text":"Initialises a decision tree model.\n\nArguments\n\nroot::Union{Node, Nothing}: the root node of the decision tree; nothing if the tree is empty\nmax_depth::Int: maximum depth of the decision tree; no limit if equal to -1\n\n\n\n\n\n","category":"method"},{"location":"#OneTwoTree.DecisionTreeRegressor","page":"Home","title":"OneTwoTree.DecisionTreeRegressor","text":"DecisionTreeRegressor\n\nA DecisionTreeRegressor is a tree of decision nodes. It can predict function values based on the input data. In addition to a root node it holds meta informations such as max_depth etc. Use fit(tree, features, labels) to create a tree from data\n\nArguments\n\nroot::Union{Node, Nothing}: the root node of the decision tree; nothing if the tree is empty\nmax_depth::Int: maximum depth of the decision tree; no limit if equal to -1\n\n\n\n\n\n","category":"type"},{"location":"#OneTwoTree.DecisionTreeRegressor-Tuple{}","page":"Home","title":"OneTwoTree.DecisionTreeRegressor","text":"Initialises a decision tree model.\n\nArguments\n\nroot::Union{Node, Nothing}: the root node of the decision tree; nothing if the tree is empty\nmax_depth::Int: maximum depth of the decision tree; no limit if equal to -1\n\n\n\n\n\n","category":"method"},{"location":"#OneTwoTree.Node","page":"Home","title":"OneTwoTree.Node","text":"Node\n\nA Node represents a decision in the Tree. It is a leaf with a prediction or has exactly one true and one false child and a decision function.\n\n\n\n\n\n","category":"type"},{"location":"#OneTwoTree._decision_to_string-Tuple{OneTwoTree.Decision}","page":"Home","title":"OneTwoTree._decision_to_string","text":"_decision_to_string(d::DecisionFn)\n\nReturns a string representation of the decision function.\n\nArguments\n\nd::DecisionFn: The decision function to convert to a string.\n\n\n\n\n\n","category":"method"},{"location":"#OneTwoTree._node_to_string-Tuple{OneTwoTree.Node, Bool, String}","page":"Home","title":"OneTwoTree._node_to_string","text":"_node_to_string(node::Node, prefix::String, is_true_child::Bool, indentation::String)\n\nRecursive helper function to stringify the decision tree structure.\n\nArguments\n\nnode: The current node to print.\nis_true_child: Boolean indicating if the node is a true branch child.\nindentation: The current indentation.\n\n\n\n\n\n","category":"method"},{"location":"#OneTwoTree._tree_to_string","page":"Home","title":"OneTwoTree._tree_to_string","text":"tree_to_string(tree::AbstractDecisionTree)\n\nReturns a textual visualization of the decision tree.\n\nArguments\n\ntree::AbstractDecisionTree The DecisionTree instance to print.\n\nExample output:\n\nx < 28.0 ? ├─ False: x == 161.0 ? │  ├─ False: 842 │  └─ True: 2493 └─ True: 683\n\n\n\n\n\n","category":"function"},{"location":"#OneTwoTree._verify_fit!_args-NTuple{4, Any}","page":"Home","title":"OneTwoTree._verify_fit!_args","text":"Some guards to ensure the input data is valid for training a tree.\n\n\n\n\n\n","category":"method"},{"location":"#OneTwoTree.calc_accuracy-Tuple{Any, Any}","page":"Home","title":"OneTwoTree.calc_accuracy","text":"calc_accuracy(labels, predictions)\n\nCalculates the accuracy of the predictions compared to the labels.\n\n\n\n\n\n","category":"method"},{"location":"#OneTwoTree.calc_depth-Tuple{AbstractDecisionTree}","page":"Home","title":"OneTwoTree.calc_depth","text":"depth(tree)\n\nTraverses the tree and returns the maximum depth.\n\n\n\n\n\n","category":"method"},{"location":"#OneTwoTree.collect_classes-Tuple{AbstractMatrix, Int64}","page":"Home","title":"OneTwoTree.collect_classes","text":"collect_classes(dataset, column)\n\nCollect all unique classes among the specified column of the dataset.\n\nArguments\n\ndataset::AbstractMatrix: the dataset to collect classes on\ncolumn::Int64: the index of the dataset column/feature to collect the classes on\n\n\n\n\n\n","category":"method"},{"location":"#OneTwoTree.collect_classes-Tuple{AbstractMatrix, Vector{Int64}, Int64}","page":"Home","title":"OneTwoTree.collect_classes","text":"collect_classes(dataset, column)\n\nCollect all unique classes among a subset of the specified column of the dataset.\n\nArguments\n\ndataset::AbstractMatrix: the dataset to collect classes on\nindices::Vector{Int64}: the indices of the numeric labels to be considered\ncolumn::Int64: the index of the dataset column/feature to collect the classes on\n\n\n\n\n\n","category":"method"},{"location":"#OneTwoTree.equal-Tuple{Any, String}","page":"Home","title":"OneTwoTree.equal","text":"equal\n\nA basic categorical decision function for testing and playing around.\n\n\n\n\n\n","category":"method"},{"location":"#OneTwoTree.fit!-Union{Tuple{T}, Tuple{AbstractDecisionTree, AbstractMatrix, Vector{T}}, Tuple{AbstractDecisionTree, AbstractMatrix, Vector{T}, Any}} where T<:Union{Number, String}","page":"Home","title":"OneTwoTree.fit!","text":"fit!(tree, features, labels)\n\nTrain a decision tree on the given data using some algorithm (e.g. CART).\n\nArguments\n\ntree::AbstractDecisionTree: the tree to be trained\ndataset::AbstractMatrix: the training data\nlabels::Vector{Union{Real, String}}: the target labels\ncolumn_data::Bool: whether the datapoints are contained in dataset columnwise\n\n\n\n\n\n","category":"method"},{"location":"#OneTwoTree.information_gain-Tuple{AbstractVector, AbstractVector, AbstractVector}","page":"Home","title":"OneTwoTree.information_gain","text":"information_gain(features::AbstractVector) -> Float64\n\nThis function calculates the information gain spliting criterion\n\nArguments:\n\nparent_labels: A vector of all considerd labels to be split\nchild_1_labels: A vector of labels of the first spliting part\nchild_2_labels: A vector of labels of the other spliting part\n\nReturns:\n\nThe information gain for given split as Float64. \ncalculated as follows:   Information gain = entropy(parent) - [weightes] * entropy(children) \n\n\n\n\n\n","category":"method"},{"location":"#OneTwoTree.is_leaf-Tuple{OneTwoTree.Node}","page":"Home","title":"OneTwoTree.is_leaf","text":"is_leaf(node)\n\nDo you seriously expect a description for this?\n\n\n\n\n\n","category":"method"},{"location":"#OneTwoTree.label_mean-Union{Tuple{T}, Tuple{Vector{T}, Vector{Int64}}} where T<:Real","page":"Home","title":"OneTwoTree.label_mean","text":"label_mean(labels, indices)\n\nCalculate the mean of a subset of numeric labels.\n\nlabels::Vector{Real}: the vector of numeric labels\nindices::Vector{Int64}: the indices of the numeric labels to be considered\n\n\n\n\n\n","category":"method"},{"location":"#OneTwoTree.less_than_or_equal-Tuple{Any, Float64}","page":"Home","title":"OneTwoTree.less_than_or_equal","text":"less_than_or_equal\n\nA basic numerical decision function for testing and playing around.\n\n\n\n\n\n","category":"method"},{"location":"#OneTwoTree.load_data-Tuple{Any}","page":"Home","title":"OneTwoTree.load_data","text":"load_data(name::String)\n\nLoad a preconfigured dataset from a CSV file.\n\nArguments\n\nname::String: the name of the dataset to load\n\nExample\n\nload_data(\"fashion_mnist_1000\")\n\n\n\n\n\n","category":"method"},{"location":"#OneTwoTree.most_frequent_class-Union{Tuple{T}, Tuple{Vector{T}, Vector{Int64}}} where T<:Union{Int64, String}","page":"Home","title":"OneTwoTree.most_frequent_class","text":"most_frequent_class(labels, indices)\n\nDetermine the most frequent class among a subset of class labels.\n\nArguments\n\nlabels::Vector{Union{Int, String}}: class labels\nindices::Vector{Int64}: the indices of the class labels, to be considered/counted\n\n\n\n\n\n","category":"method"},{"location":"#OneTwoTree.predict-Tuple{AbstractDecisionTree, Union{AbstractMatrix, AbstractVector}}","page":"Home","title":"OneTwoTree.predict","text":"predict\n\nTraverses the tree for a given datapoint x and returns that trees prediction.\n\nArguments\n\ntree::AbstractDecisionTree: the tree to predict with\nX::Union{AbstractMatrix, AbstractVector: the data to predict on\n\n\n\n\n\n","category":"method"},{"location":"#OneTwoTree.printM-Union{Tuple{Matrix{S}}, Tuple{S}} where S<:Union{Real, String}","page":"Home","title":"OneTwoTree.printM","text":"printM(M)\n\nPrint the matrix M:\n\nArguments\n\nM::Matrix{Union{Real, String}}\n\n\n\n\n\n","category":"method"},{"location":"#OneTwoTree.print_tree-Tuple{AbstractDecisionTree}","page":"Home","title":"OneTwoTree.print_tree","text":"print_tree(tree::AbstractDecisionTree)\n\nReturns a textual visualization of the decision tree.\n\nArguments\n\ntree::AbstractDecisionTree The DecisionTree instance to print.\n\nExample output:\n\nx < 28.0 ? ├─ False: x == 161.0 ? │  ├─ False: 842 │  └─ True: 2493 └─ True: 683\n\n\n\n\n\n","category":"method"},{"location":"#OneTwoTree.should_split-Tuple{OneTwoTree.Node, Float64, Int64}","page":"Home","title":"OneTwoTree.should_split","text":"should_split(N, post_split_impurity, max_depth)\n\nDetermines whether to split the node N given.\n\nArguments\n\nN::Node: Node that may be split. N contains further fields relevant to the decision like the best splitting decision function, it's leaf impurity and depth.\npost_split_impurity::Float64: The impurity of N after it's optimal split.\nmax_depth::Int64: The maximum depth of the tree N is part of.\n\n\n\n\n\n","category":"method"},{"location":"#OneTwoTree.split-Tuple{OneTwoTree.Node}","page":"Home","title":"OneTwoTree.split","text":"split(N)\n\nDetermine the optimal split of a node. Handles numerical and categorical data and labels.\n\nArguments\n\nN::Node: The node to be split. All additional information for the split calculation (e.g. dataset, labels, node_data) is contained in N.\n\n\n\n\n\n","category":"method"},{"location":"#OneTwoTree.split_indices-Tuple{AbstractMatrix, Vector{Int64}, Function}","page":"Home","title":"OneTwoTree.split_indices","text":"split_indices(dataset, node_data, decision_fn)\n\nSplit the dataset indices contained in node_data into two sets via the decision function.\n\nArguments\n\ndataset::AbstractMatrix: the dataset to split on (the datapoints are assumed to be contained rowwise)\nnode_data::Vector{Int64}: the index list, indexing the dataset, to be split\ndecision_fn::Function: the decision function taking as input a datapoint and returning a Bool\n\n\n\n\n\n","category":"method"},{"location":"#OneTwoTree.split_indices-Union{Tuple{T}, Tuple{AbstractMatrix, Vector{Int64}, Function, T, Int64}} where T<:Union{Real, String}","page":"Home","title":"OneTwoTree.split_indices","text":"split_indices(dataset, node_data, decision_fn, decision_param, decision_feature)\n\nSplit the dataset indices contained in node_data into two sets via the decision function.\n\nArguments\n\ndataset::AbstractMatrix: the dataset to split on (the datapoints are assumed to be contained rowwise)\nnode_data::Vector{Int64}: the index list, indexing dataset, to be split\ndecision_fn::Function: the decision function taking as input a datapoint, decisionparam and decisionfeature and returning a Bool\ndecision_param::Union{Real, String}: a parameter to the decision function. This can be a class name or a numeric threshold.\ndecision_feature::Int64: the index of the dimension of datapoints along which to split\n\n\n\n\n\n","category":"method"}]
}
