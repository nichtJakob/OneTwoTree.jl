<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Functions · OneTwoTree.jl</title><meta name="title" content="Functions · OneTwoTree.jl"/><meta property="og:title" content="Functions · OneTwoTree.jl"/><meta property="twitter:title" content="Functions · OneTwoTree.jl"/><meta name="description" content="Documentation for OneTwoTree.jl."/><meta property="og:description" content="Documentation for OneTwoTree.jl."/><meta property="twitter:description" content="Documentation for OneTwoTree.jl."/><meta property="og:url" content="https://nichtJakob.github.io/OneTwoTree.jl/functions/"/><meta property="twitter:url" content="https://nichtJakob.github.io/OneTwoTree.jl/functions/"/><link rel="canonical" href="https://nichtJakob.github.io/OneTwoTree.jl/functions/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">OneTwoTree.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../getting_started/">Getting Started</a></li><li class="is-active"><a class="tocitem" href>Functions</a></li><li><a class="tocitem" href="../demo_classification/">Demo Classification</a></li><li><a class="tocitem" href="../demo_regression/">Demo Regression</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Functions</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Functions</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/nichtJakob/OneTwoTree.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/nichtJakob/OneTwoTree.jl/blob/master/docs/src/functions.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><ul><li><a href="#OneTwoTree.DecisionTreeClassifier"><code>OneTwoTree.DecisionTreeClassifier</code></a></li><li><a href="#OneTwoTree.DecisionTreeClassifier-Tuple{}"><code>OneTwoTree.DecisionTreeClassifier</code></a></li><li><a href="#OneTwoTree.DecisionTreeRegressor-Tuple{}"><code>OneTwoTree.DecisionTreeRegressor</code></a></li><li><a href="#OneTwoTree.DecisionTreeRegressor"><code>OneTwoTree.DecisionTreeRegressor</code></a></li><li><a href="#OneTwoTree.ForestClassifier"><code>OneTwoTree.ForestClassifier</code></a></li><li><a href="#OneTwoTree.ForestClassifier-Tuple{}"><code>OneTwoTree.ForestClassifier</code></a></li><li><a href="#OneTwoTree.ForestRegressor-Tuple{}"><code>OneTwoTree.ForestRegressor</code></a></li><li><a href="#OneTwoTree.ForestRegressor"><code>OneTwoTree.ForestRegressor</code></a></li><li><a href="#OneTwoTree.calc_accuracy-Tuple{AbstractArray, AbstractArray}"><code>OneTwoTree.calc_accuracy</code></a></li><li><a href="#OneTwoTree.fit!-Union{Tuple{T}, Tuple{S}, Tuple{OneTwoTree.AbstractDecisionTree, AbstractMatrix{S}, AbstractVector{T}}} where {S, T&lt;:Union{Real, String}}"><code>OneTwoTree.fit!</code></a></li><li><a href="#OneTwoTree.fit!-Union{Tuple{T}, Tuple{S}, Tuple{OneTwoTree.AbstractForest, AbstractMatrix{S}, AbstractVector{T}}} where {S, T&lt;:Union{Real, String}}"><code>OneTwoTree.fit!</code></a></li><li><a href="#OneTwoTree.gini_gain-Tuple{AbstractVector, AbstractVector, AbstractVector}"><code>OneTwoTree.gini_gain</code></a></li><li><a href="#OneTwoTree.information_gain-Tuple{AbstractVector, AbstractVector, AbstractVector}"><code>OneTwoTree.information_gain</code></a></li><li><a href="#OneTwoTree.predict-Tuple{OneTwoTree.AbstractDecisionTree, Union{AbstractMatrix, AbstractVector}}"><code>OneTwoTree.predict</code></a></li><li><a href="#OneTwoTree.predict-Union{Tuple{S}, Tuple{OneTwoTree.AbstractForest, AbstractVecOrMat{S}}} where S&lt;:Union{Real, String}"><code>OneTwoTree.predict</code></a></li><li><a href="#OneTwoTree.print_forest-Tuple{OneTwoTree.AbstractForest}"><code>OneTwoTree.print_forest</code></a></li><li><a href="#OneTwoTree.print_tree-Tuple{OneTwoTree.AbstractDecisionTree}"><code>OneTwoTree.print_tree</code></a></li><li><a href="#OneTwoTree.variance_gain-Tuple{AbstractVector, AbstractVector, AbstractVector}"><code>OneTwoTree.variance_gain</code></a></li></ul><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OneTwoTree.DecisionTreeClassifier" href="#OneTwoTree.DecisionTreeClassifier"><code>OneTwoTree.DecisionTreeClassifier</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">DecisionTreeClassifier &lt;: AbstractDecisionTree</code></pre><p>A DecisionTreeClassifier is a tree of decision nodes. It can predict classes based on the input data. In addition to a root node it holds meta informations such as max_depth etc. Use <code>fit(tree, features, labels)</code> to create a tree from data.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/nichtJakob/OneTwoTree.jl/blob/f921f2d4c027a2e22ec7611143126a1746b98ed3/src/trees/tree.jl#L9-L15">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OneTwoTree.DecisionTreeClassifier-Tuple{}" href="#OneTwoTree.DecisionTreeClassifier-Tuple{}"><code>OneTwoTree.DecisionTreeClassifier</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">DecisionTreeClassifier(; root=nothing, max_depth=-1)</code></pre><p>Initialises a decision tree model.</p><p><strong>Arguments</strong></p><ul><li><code>root::Union{Node, Nothing}</code>: the root node of the decision tree; <code>nothing</code> if the tree is empty</li><li><code>max_depth::Int</code>: maximum depth of the decision tree; no limit if equal to -1</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/nichtJakob/OneTwoTree.jl/blob/f921f2d4c027a2e22ec7611143126a1746b98ed3/src/trees/tree.jl#L21-L30">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OneTwoTree.DecisionTreeRegressor" href="#OneTwoTree.DecisionTreeRegressor"><code>OneTwoTree.DecisionTreeRegressor</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">DecisionTreeRegressor</code></pre><p>A DecisionTreeRegressor is a tree of decision nodes. It can predict function values based on the input data. In addition to a root node it holds meta informations such as max_depth etc. Use <code>fit(tree, features, labels)</code> to create a tree from data.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/nichtJakob/OneTwoTree.jl/blob/f921f2d4c027a2e22ec7611143126a1746b98ed3/src/trees/tree.jl#L38-L44">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OneTwoTree.DecisionTreeRegressor-Tuple{}" href="#OneTwoTree.DecisionTreeRegressor-Tuple{}"><code>OneTwoTree.DecisionTreeRegressor</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">DecisionTreeRegressor(; root=nothing, max_depth=-1)</code></pre><p>Initialises a decision tree model.</p><p><strong>Arguments</strong></p><ul><li><code>root::Union{Node, Nothing}</code>: the root node of the decision tree; <code>nothing</code> if the tree is empty</li><li><code>max_depth::Int</code>: maximum depth of the decision tree; no limit if equal to -1</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/nichtJakob/OneTwoTree.jl/blob/f921f2d4c027a2e22ec7611143126a1746b98ed3/src/trees/tree.jl#L50-L59">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OneTwoTree.ForestClassifier" href="#OneTwoTree.ForestClassifier"><code>OneTwoTree.ForestClassifier</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">ForestClassifier &lt;: AbstractForest</code></pre><p>Forest for classification problems</p><p><strong>Fields:</strong></p><ul><li><code>trees::Vector{DecisionTreeClassifier}</code>: A vector of decision trees.</li><li><code>n_trees::Int</code>: The number of trees in the forest.</li><li><code>n_features_per_tree::Int</code>: The number of randomly drawn features used for each tree.</li><li><code>max_depth::Int</code>: The upper bound for the depth of each tree.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/nichtJakob/OneTwoTree.jl/blob/f921f2d4c027a2e22ec7611143126a1746b98ed3/src/trees/forest.jl#L11-L21">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OneTwoTree.ForestClassifier-Tuple{}" href="#OneTwoTree.ForestClassifier-Tuple{}"><code>OneTwoTree.ForestClassifier</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">ForestClassifier(;n_trees::Int, n_features_per_tree::Int, max_depth::Int)</code></pre><p>Constructs a ForestClassifier instance.</p><p><strong>Arguments:</strong></p><ul><li><code>n_trees::Int</code>: The number of trees in the forest.</li><li><code>n_features_per_tree::Int</code>: The number of randomly drawn features used for each tree.</li><li><code>max_depth::Int</code>: The upper bound for the depth of each tree.</li></ul><p><strong>Returns:</strong></p><p>A <code>ForestClassifier</code> instance.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/nichtJakob/OneTwoTree.jl/blob/f921f2d4c027a2e22ec7611143126a1746b98ed3/src/trees/forest.jl#L74-L86">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OneTwoTree.ForestRegressor" href="#OneTwoTree.ForestRegressor"><code>OneTwoTree.ForestRegressor</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">ForestRegressor &lt;: AbstractForest</code></pre><p>Forest for regression problems</p><p><strong>Fields:</strong></p><ul><li><code>trees::Vector{DecisionTreeRegressor}</code>: A vector of regression trees.</li><li><code>n_trees::Int</code>: The number of trees in the forest.</li><li><code>n_features_per_tree::Int</code>: The number of randomly drawn features used for each tree.</li><li><code>max_depth::Int</code>: The upper bound for the depth of each tree.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/nichtJakob/OneTwoTree.jl/blob/f921f2d4c027a2e22ec7611143126a1746b98ed3/src/trees/forest.jl#L29-L39">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OneTwoTree.ForestRegressor-Tuple{}" href="#OneTwoTree.ForestRegressor-Tuple{}"><code>OneTwoTree.ForestRegressor</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">ForestRegressor(;n_trees::Int, n_features_per_tree::Int, max_depth::Int)</code></pre><p>Constructs a ForestRegressor instance.</p><p><strong>Arguments:</strong></p><ul><li><code>n_trees::Int</code>: The number of trees in the forest.</li><li><code>n_features_per_tree::Int</code>: The number of randomly drawn features used for each tree.</li><li><code>max_depth::Int</code>: The upper bound for the depth of each tree.</li></ul><p><strong>Returns:</strong></p><p>A <code>ForestRegressor</code> instance.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/nichtJakob/OneTwoTree.jl/blob/f921f2d4c027a2e22ec7611143126a1746b98ed3/src/trees/forest.jl#L93-L105">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OneTwoTree.calc_accuracy-Tuple{AbstractArray, AbstractArray}" href="#OneTwoTree.calc_accuracy-Tuple{AbstractArray, AbstractArray}"><code>OneTwoTree.calc_accuracy</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">calc_accuracy(labels::AbstractArray, predictions::AbstractArray)</code></pre><p>Calculates the accuracy of the predictions compared to the labels.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/nichtJakob/OneTwoTree.jl/blob/f921f2d4c027a2e22ec7611143126a1746b98ed3/src/trees/tree.jl#L186-L190">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OneTwoTree.fit!-Union{Tuple{T}, Tuple{S}, Tuple{OneTwoTree.AbstractDecisionTree, AbstractMatrix{S}, AbstractVector{T}}} where {S, T&lt;:Union{Real, String}}" href="#OneTwoTree.fit!-Union{Tuple{T}, Tuple{S}, Tuple{OneTwoTree.AbstractDecisionTree, AbstractMatrix{S}, AbstractVector{T}}} where {S, T&lt;:Union{Real, String}}"><code>OneTwoTree.fit!</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">fit!(tree::AbstractDecisionTree, features::AbstractMatrix{S}, labels::AbstractVector{T}; splitting_criterion=nothing, column_data=false) where {S, T&lt;:Union{Real, String}}</code></pre><p>Train a decision tree on the given data using some algorithm (e.g. CART).</p><p><strong>Arguments</strong></p><ul><li><code>tree::AbstractDecisionTree</code>: the tree to be trained</li><li><code>dataset::AbstractMatrix{S}</code>: the training data</li><li><code>labels::AbstractVector{T}</code>: the target labels</li><li><code>splitting_criterion::Function</code>: a function indicating some notion of gain from splitting a node. If not provided, default criteria for classification and regression are used.</li><li><code>column_data::Bool</code>: whether the datapoints are contained in dataset columnwise</li></ul><p>(OneTwoTree provides the following splitting criteria for classification: gini<em>gain, information</em>gain; and for regression: variance_gain. If you&#39;d like to define a splitting criterion yourself, you need to consider the following:</p><ol><li>The function must calculate a &#39;gain&#39;-value for a split of a node, meaning that larger values are considered better.</li><li>The function signature must conform to <code>my_func(parent_labels::AbstractVector, true_child_labels::AbstractVector, false_child_labels::AbstractVector)</code>,</li></ol><p>where <code>parent_labels</code> is a set of datapoint labels, which is split into two subsets <code>true_child_labels</code> &amp; <code>false_child_labels</code> by some discriminating function. (Each label in <code>parent_labels</code> is contained in exactly one of the two subsets.)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/nichtJakob/OneTwoTree.jl/blob/f921f2d4c027a2e22ec7611143126a1746b98ed3/src/trees/tree.jl#L104-L121">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OneTwoTree.fit!-Union{Tuple{T}, Tuple{S}, Tuple{OneTwoTree.AbstractForest, AbstractMatrix{S}, AbstractVector{T}}} where {S, T&lt;:Union{Real, String}}" href="#OneTwoTree.fit!-Union{Tuple{T}, Tuple{S}, Tuple{OneTwoTree.AbstractForest, AbstractMatrix{S}, AbstractVector{T}}} where {S, T&lt;:Union{Real, String}}"><code>OneTwoTree.fit!</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">fit!(forest::AbstractForest, dataset::AbstractMatrix{S}, labels::AbstractVector{T}; splitting_criterion=nothing, column_data=false) where {S, T&lt;:Union{Real, String}}</code></pre><p>Trains each tree in the forest on randomly drawn subsets of test features and corresponding test labels.</p><p><strong>Arguments</strong></p><ul><li><code>forest::AbstractForest</code>: the forest to be trained</li><li><code>dataset::AbstractMatrix{S}</code>: the training data</li><li><code>labels::AbstractVector{T}</code>: the target labels</li><li><code>splitting_criterion</code>: a function indicating some notion of gain from splitting a node. If not provided, default criteria for classification and regression are used.</li><li><code>column_data::Bool</code>: whether the datapoints are contained in dataset columnwise</li></ul><p>(OneTwoTree provides the following splitting criteria for classification: gini<em>gain, information</em>gain; and for regression: variance_gain. If you&#39;d like to define a splitting criterion yourself, you need to consider the following:</p><ol><li>The function must calculate a &#39;gain&#39;-value for a split of a node, meaning that larger values are considered better.</li><li>The function signature must conform to <code>my_func(parent_labels::AbstractVector, true_child_labels::AbstractVector, false_child_labels::AbstractVector)</code>,</li></ol><p>where <code>parent_labels</code> is a set of datapoint labels, which is split into two subsets <code>true_child_labels</code> &amp; <code>false_child_labels</code> by some discriminating function. (Each label in <code>parent_labels</code> is contained in exactly one of the two subsets.)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/nichtJakob/OneTwoTree.jl/blob/f921f2d4c027a2e22ec7611143126a1746b98ed3/src/trees/forest.jl#L133-L150">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OneTwoTree.gini_gain-Tuple{AbstractVector, AbstractVector, AbstractVector}" href="#OneTwoTree.gini_gain-Tuple{AbstractVector, AbstractVector, AbstractVector}"><code>OneTwoTree.gini_gain</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">gini_gain(parent_labels::AbstractVector, true_child_labels::AbstractVector, false_child_labels::AbstractVector)::Float64</code></pre><p>This function calculates the gain in Gini impurity for a split in a decision tree. The split is characterized by the partition of the parent<em>labels into true</em>child<em>labels and false</em>child_labels according to some discriminant function.</p><p><strong>Arguments:</strong></p><ul><li><code>parent_labels</code>: A vector of data labels (e.g., classes or numerical values in the case of regression).</li><li><code>true_child_labels</code>: A vector of a subset of data labels contained in parent_labels.</li><li><code>false_child_labels</code>: A vector of a subset of data labels contained in parent_labels.</li></ul><p><strong>Returns:</strong></p><ul><li>The gain in Gini impurity of the split.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/nichtJakob/OneTwoTree.jl/blob/f921f2d4c027a2e22ec7611143126a1746b98ed3/src/splitting_criteria/gini.jl#L1-L13">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OneTwoTree.information_gain-Tuple{AbstractVector, AbstractVector, AbstractVector}" href="#OneTwoTree.information_gain-Tuple{AbstractVector, AbstractVector, AbstractVector}"><code>OneTwoTree.information_gain</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">information_gain(parent_labels::AbstractVector, true_child_labels::AbstractVector, false_child_labels::AbstractVector)::Float64</code></pre><p>This function calculates the information gain for a split in a decision tree. The split is characterized by the partition of the parent<em>labels into true</em>child<em>labels and false</em>child_labels according to some discriminant function.</p><p><strong>Arguments:</strong></p><ul><li><code>parent_labels</code>: A vector of data labels (e.g., classes or numerical values in the case of regression).</li><li><code>true_child_labels</code>: A vector of a subset of data labels contained in parent_labels.</li><li><code>false_child_labels</code>: A vector of a subset of data labels contained in parent_labels.</li></ul><p><strong>Returns:</strong></p><ul><li>The information gain of the split.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/nichtJakob/OneTwoTree.jl/blob/f921f2d4c027a2e22ec7611143126a1746b98ed3/src/splitting_criteria/info_gain.jl#L22-L34">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OneTwoTree.predict-Tuple{OneTwoTree.AbstractDecisionTree, Union{AbstractMatrix, AbstractVector}}" href="#OneTwoTree.predict-Tuple{OneTwoTree.AbstractDecisionTree, Union{AbstractMatrix, AbstractVector}}"><code>OneTwoTree.predict</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">predict(tree::AbstractDecisionTree, X::Union{AbstractMatrix, AbstractVector})</code></pre><p>Traverses the tree for given datapoints and returns that trees prediction.</p><p><strong>Arguments</strong></p><ul><li><code>tree::AbstractDecisionTree</code>: the tree to predict with</li><li><code>X::Union{AbstractMatrix, AbstractVector}</code>: the data to predict on</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/nichtJakob/OneTwoTree.jl/blob/f921f2d4c027a2e22ec7611143126a1746b98ed3/src/trees/tree.jl#L136-L145">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OneTwoTree.predict-Union{Tuple{S}, Tuple{OneTwoTree.AbstractForest, AbstractVecOrMat{S}}} where S&lt;:Union{Real, String}" href="#OneTwoTree.predict-Union{Tuple{S}, Tuple{OneTwoTree.AbstractForest, AbstractVecOrMat{S}}} where S&lt;:Union{Real, String}"><code>OneTwoTree.predict</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">predict(forest::AbstractForest, X::Union{AbstractMatrix{S}, AbstractVector{S}}) where S&lt;:Union{Real, String}</code></pre><p>Outputs the forest-prediction for a given datapoint X. The prediction is based on the aggregation of the tree decisions. For aggregation in a regression scenario the mean is used. For aggregation in a classification scenario the most voted class label is used.</p><p><strong>Arguments:</strong></p><ul><li><code>forest::AbstractForest</code>: The trained forest.</li><li><code>X::Union{AbstractMatrix{S}, AbstractVector{S}}</code>: The input data for which a prediction is searched.</li></ul><p><strong>Returns:</strong></p><p>Predictions for the input data X, aggregated across all trees in the forest.</p><p><strong>Errors:</strong></p><p>Raises an error if the forest contains no trained trees.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/nichtJakob/OneTwoTree.jl/blob/f921f2d4c027a2e22ec7611143126a1746b98ed3/src/trees/forest.jl#L169-L186">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OneTwoTree.print_forest-Tuple{OneTwoTree.AbstractForest}" href="#OneTwoTree.print_forest-Tuple{OneTwoTree.AbstractForest}"><code>OneTwoTree.print_forest</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">print_forest(forest::AbstractForest; io::IO=stdout)</code></pre><p>Prints the numerated trees of a forest.</p><p><strong>Arguments:</strong></p><ul><li><code>forest::AbstractForest</code>: The forest to be printed.</li><li><code>io::IO=stdout</code>: (Optional) The I/O stream for printing</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/nichtJakob/OneTwoTree.jl/blob/f921f2d4c027a2e22ec7611143126a1746b98ed3/src/trees/forest.jl#L222-L230">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OneTwoTree.print_tree-Tuple{OneTwoTree.AbstractDecisionTree}" href="#OneTwoTree.print_tree-Tuple{OneTwoTree.AbstractDecisionTree}"><code>OneTwoTree.print_tree</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">print_tree(tree::AbstractDecisionTree; io::IO=stdout)</code></pre><p>Returns a textual visualization of the decision tree.</p><p><strong>Arguments</strong></p><ul><li><code>tree::AbstractDecisionTree</code> The <code>DecisionTree</code> instance to print.</li><li><code>io::IO=stdout</code>: (Optional) The I/O stream for printing</li></ul><p><strong>Example output:</strong></p><pre><code class="nohighlight hljs">x &lt; 28.0 ?
├─ False: x == 161.0 ?
│  ├─ False: 842
│  └─ True: 2493
└─ True: 683</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/nichtJakob/OneTwoTree.jl/blob/f921f2d4c027a2e22ec7611143126a1746b98ed3/src/trees/tree.jl#L280-L298">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="OneTwoTree.variance_gain-Tuple{AbstractVector, AbstractVector, AbstractVector}" href="#OneTwoTree.variance_gain-Tuple{AbstractVector, AbstractVector, AbstractVector}"><code>OneTwoTree.variance_gain</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">variance_gain(parent_labels::AbstractVector, true_child_labels::AbstractVector, false_child_labels::AbstractVector)::Float64</code></pre><p>This function calculates the variance gain for a split in a decision tree. The split is characterized by the partition of the parent<em>labels into true</em>child<em>labels and false</em>child_labels according to some discriminant function.</p><p><strong>Arguments:</strong></p><ul><li><code>parent_labels</code>: A vector of data labels (e.g., classes or numerical values in the case of regression).</li><li><code>true_child_labels</code>: A vector of a subset of data labels contained in parent_labels.</li><li><code>false_child_labels</code>: A vector of a subset of data labels contained in parent_labels.</li></ul><p><strong>Returns:</strong></p><ul><li>The variance gain of the split.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/nichtJakob/OneTwoTree.jl/blob/f921f2d4c027a2e22ec7611143126a1746b98ed3/src/splitting_criteria/var_gain.jl#L11-L23">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../getting_started/">« Getting Started</a><a class="docs-footer-nextpage" href="../demo_classification/">Demo Classification »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.8.0 on <span class="colophon-date" title="Friday 31 January 2025 18:36">Friday 31 January 2025</span>. Using Julia version 1.11.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
